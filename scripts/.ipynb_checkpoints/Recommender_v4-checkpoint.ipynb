{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "threatened-judge",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import pandas as pd\n",
    "import glob\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "import json\n",
    "import csv\n",
    "import matplotlib\n",
    "from operator import itemgetter\n",
    "import psutil\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from math import isnan\n",
    "import numpy\n",
    "import ntpath\n",
    "import git\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from boutiques.searcher import Searcher\n",
    "from boutiques.puller import Puller\n",
    "from github import Github\n",
    "import matplotlib.pyplot as plt; plt.rcdefaults()\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import ImageColor\n",
    "from statistics import mean\n",
    "from scipy.stats import ttest_ind\n",
    "from statistics import mean\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "\n",
    "inputsFolder = os.path.join(os.getcwd(), \"..\", \"data\")\n",
    "outputsFolder = os.path.join(os.getcwd(), \"..\", \"result\")\n",
    "if not os.path.exists(outputsFolder):\n",
    "    os.mkdir(outputsFolder) \n",
    "    \n",
    "figuresFolder = os.path.join(os.getcwd(), \"..\", \"figures\")\n",
    "if not os.path.exists(figuresFolder):\n",
    "    os.mkdir(figuresFolder) \n",
    "    \n",
    "expertsFolder = os.path.join(inputsFolder, \"Expert Comments\")\n",
    "\n",
    "recommender_DF_FN = os.path.join(outputsFolder, \"Recommender DF v4.xlsx\")\n",
    "recommender_JSON_FN = recommender_DF_FN.replace(\".xlsx\", \".json\")\n",
    "\n",
    "RECOMMENDER_DF_STATISTICS = {\n",
    "    \"Num of all items\":0,\n",
    "    #100% success\n",
    "    \"Num of Assessed By Experts\": 0,       #Listed in Experts assessment\n",
    "    \"Num of Not Assessed By Experts\": 0,   #Not listed in Experts assessment\n",
    "    \"Num of Success Predicted By Experts\": 0,\n",
    "    \n",
    "    \"Num of Tests\": 0,\n",
    "    \"Num of Tests is failed\": 0,\n",
    "    \"Num of Tests is wrong data or wrong format\": 0,\n",
    "    \"Num of Tests is not available\": 0,\n",
    "    \"Num of Tests is pipeline issue\": 0,\n",
    "    \"Num of Tests is success\": 0,\n",
    "    \"Num of Tests is not tested\": 0,\n",
    "    \n",
    "    \"Num of 100% Assessed Success\": 0,\n",
    "    \n",
    "    \"Num of Cells With Comment\": 0,\n",
    "    \"Num of Total Comments\": 0,\n",
    "    \"Average Expert Comments per Cell\": 0,\n",
    "}\n",
    "\n",
    "DATA_TEMPLATE = {\n",
    "    \"EXPERT_COMMENTS\": [],\n",
    "    \"EXPERTS_COMMENTED_SUCCESS\": [],\n",
    "    \"NUM_OF_EXPERT_COMMENTS\": 0,\n",
    "    \"EXPERT_CONFIDENCE_LIST\": [],\n",
    "    \"CONFIDENCE_RATING\": 0,\n",
    "    \"CONFIDENCE_SUCCESS_RATING\": 0,\n",
    "    \"EXECUTION_RESULT_DETAILS\": \"\",\n",
    "    \"EXECUTION_RESULT_BRIEF\": \"\",\n",
    "    \"FRACTION_RATE_SUCCESS_OVER_ALL\": 0,\n",
    "    \"FRACTION_RATE_SUCCESS_OVER_COMMENTED\": 0,\n",
    "    \n",
    "}\n",
    "\n",
    "executionResult_DF_FN = os.path.join(inputsFolder, \"Provenance Records.xlsx\")\n",
    "REAL_TESTS_RESULT_CODES = {\n",
    "    'failed': 5, \n",
    "    'wrong data': 4, \n",
    "    'wrong format': 4, \n",
    "    'not available': 3, \n",
    "    'pipeline issue': 2,\n",
    "    'success': 1\n",
    "    #' ': 0\n",
    "    }\n",
    "\n",
    "#The file that stores the confidence level of Experts\n",
    "surveyInputFile = os.path.join(inputsFolder, \"Experts Assessment.csv\")\n",
    "asessedPipelinesFile = os.path.join(inputsFolder, \"Assessed Pipelines.txt\")\n",
    "assessedDatasetFile = os.path.join(inputsFolder, \"Assessed Datasets.txt\")\n",
    "\n",
    "#To rate the confidence of the Experts\n",
    "confidenceRatings = {'No idea':1, 'Some idea': 2, 'Good knowledge': 3, 'Expert': 4}\n",
    "\n",
    "#TO store the Indexes for Pipelines and Datasets\n",
    "pipelineIndexTable_DF_FN = os.path.join(outputsFolder, \"Pipeline Index Table v4.xlsx\")\n",
    "datasetIndexTable_DF_FN = os.path.join(outputsFolder, \"Dataset Index Table v4.xlsx\")\n",
    "confidenceEvaluationReport_DF_FN = os.path.join(outputsFolder, \"Confidence Evaluation Report.csv\")\n",
    "fractionRateReport_DF_FN = os.path.join(outputsFolder, \"Fraction Rate Report.csv\")\n",
    "\n",
    "pipelineDescriptors_FN = os.path.join(inputsFolder, \"CONP Descriptors\", \"all_descriptors.json\")\n",
    "\n",
    "alsModelInput_DF_FN = os.path.join(inputsFolder, \"ALS Model\", 'ALS Model Input.csv')\n",
    "alsModel_FN = os.path.join(inputsFolder, \"ALS Model\", 'All Predicted Values Test Set.csv')\n",
    "\n",
    "#The Global Dataframe that stores everything\n",
    "recommender_DF = pd.DataFrame()\n",
    "\n",
    "\n",
    "#Loads data from DataFrame (recommender_DF_FN) to recommender_DF\n",
    "def loadRecommenderDataFrame(): \n",
    "    global recommender_DF\n",
    "    \n",
    "    #Load JSON to DataFrame\n",
    "    recommender_DF = pd.read_json(recommender_JSON_FN)\n",
    "    recommender_DF.index.name = 'Pipelines/Datasets'\n",
    "    recommender_DF = recommender_DF.sort_values(by=['Pipelines/Datasets'])\n",
    "    recommender_DF = recommender_DF.reindex(sorted(recommender_DF.columns), axis=1)\n",
    "\n",
    "def saveRecommenderDataFrame():\n",
    "    global recommender_DF\n",
    "    if recommender_DF.empty:\n",
    "        return\n",
    "    \n",
    "    recommender_DF.index.name = 'Pipelines/Datasets'\n",
    "    recommender_DF = recommender_DF.sort_values(by=['Pipelines/Datasets'])\n",
    "    recommender_DF = recommender_DF.reindex(sorted(recommender_DF.columns), axis=1)\n",
    "    if not os.path.exists(outputsFolder):\n",
    "        os.mkdir(outputsFolder) \n",
    "    recommender_DF.to_excel(recommender_DF_FN) \n",
    "    \n",
    "    #Write JSON to file\n",
    "    with open(recommender_JSON_FN, 'w') as outfile:\n",
    "        #json.dump(recommender_Dict, outfile, indent = 4)\n",
    "        json.dump(recommender_DF.to_dict(), outfile, indent = 4)\n",
    "\n",
    "def saveReportFile(functionName, fileContentStringList):\n",
    "    reportFileName = os.path.join(outputsFolder, functionName + \".txt\")\n",
    "    reportFileHandler = open(reportFileName, 'w')\n",
    "    for item in fileContentStringList:\n",
    "        if type(item) == list:\n",
    "            item = '[' + ''.join(str(e)+', ' for e in item) + ']'\n",
    "        #print(item)\n",
    "        reportFileHandler.write(item + \"\\n\")\n",
    "    reportFileHandler.close()\n",
    "\n",
    "    \n",
    "def saveReportFileJson(functionNameJson, fileContent):\n",
    "    with open(os.path.join(outputsFolder, functionNameJson), 'w') as outfile:\n",
    "        json.dump(fileContent, outfile, indent = 4)\n",
    "        outfile.close()\n",
    "\n",
    "    \n",
    "def indexDataFrame_v4(dataFrame):\n",
    "    if dataFrame.empty:\n",
    "        return\n",
    "    \n",
    "    #Get Indexes for Pipelines\n",
    "    df = pd.read_excel(pipelineIndexTable_DF_FN)\n",
    "    df = df.set_index(df.columns[0])\n",
    "    \n",
    "    keys = df.index.values.tolist()\n",
    "    values = df.values.tolist()\n",
    "    \n",
    "    pipelineIndexDict = dict()\n",
    "    for i in range(0, len(keys)):\n",
    "        pipelineIndexDict[keys[i]] = values[i][0]\n",
    "    \n",
    "    #Get Indexes for Datasets\n",
    "    df = pd.read_excel(datasetIndexTable_DF_FN)\n",
    "    df = df.set_index(df.columns[0])\n",
    "    \n",
    "    keys = df.index.values.tolist()\n",
    "    values = df.values.tolist()\n",
    "    \n",
    "    datasetIndexDict = dict()\n",
    "    for i in range(0, len(keys)):\n",
    "        datasetIndexDict[keys[i]] = values[i][0]\n",
    "\n",
    "    #Index the main data Frame\n",
    "    dataFrame.rename(columns=datasetIndexDict, inplace=True)\n",
    "    dataFrame.rename(index=pipelineIndexDict, inplace=True)\n",
    "\n",
    "    return dataFrame\n",
    "\n",
    "\n",
    "def getPipelineIndex(pipelineName):\n",
    "    #Get Indexes for Pipelines\n",
    "    df = pd.read_excel(pipelineIndexTable_DF_FN)\n",
    "    df = df.set_index(df.columns[0])\n",
    "    return df.loc[pipelineName]\n",
    "\n",
    "def getPipelinesIndexDict():\n",
    "    #Get Indexes for Pipelines\n",
    "    df = pd.read_excel(pipelineIndexTable_DF_FN)\n",
    "    df = df.set_index(df.columns[0])\n",
    "\n",
    "    keys = df.index.values.tolist()\n",
    "    values = df.values.tolist()\n",
    "    \n",
    "    pipelineIndexDict = dict()\n",
    "    for i in range(0, len(keys)):\n",
    "        pipelineIndexDict[keys[i]] = values[i][0]\n",
    "    return pipelineIndexDict\n",
    "\n",
    "def getDatasetIndex(datasetName):\n",
    "    #Get Indexes for Datasets\n",
    "    df = pd.read_excel(datasetIndexTable_DF_FN)\n",
    "    df = df.set_index(df.columns[0])\n",
    "    return df.loc[datasetName]\n",
    "\n",
    "def getDatasetsIndexDict():\n",
    "    #Get Indexes for Datasets\n",
    "    df = pd.read_excel(datasetIndexTable_DF_FN)\n",
    "    df = df.set_index(df.columns[0])\n",
    "\n",
    "    keys = df.index.values.tolist()\n",
    "    values = df.values.tolist()\n",
    "    \n",
    "    datasetIndexDict = dict()\n",
    "    for i in range(0, len(keys)):\n",
    "        datasetIndexDict[keys[i]] = values[i][0]\n",
    "\n",
    "    return datasetIndexDict\n",
    "\n",
    "def showFigure_Ploly_v4(\n",
    "    processingFunction, \n",
    "    useIndex = False,\n",
    "    colorScale=[[0, \"red\"], [0.5 , \"white\"], [1, \"green\"]], \n",
    "    figureTitle=None,\n",
    "    result_DF_FN = None):\n",
    "    \n",
    "    global recommender_DF\n",
    "    if recommender_DF.empty:\n",
    "        loadRecommenderDataFrame()\n",
    "    \n",
    "    if not processingFunction:\n",
    "        return\n",
    "    \n",
    "    temporary_DF = pd.DataFrame(index=recommender_DF.index.tolist(), columns=recommender_DF.columns.tolist())\n",
    "    \n",
    "    for rowName in recommender_DF.index:\n",
    "        for columnName in recommender_DF.columns:\n",
    "            temporary_DF.loc[rowName, columnName] = processingFunction(recommender_DF.loc[rowName, columnName])\n",
    "    \n",
    "    if useIndex is True:\n",
    "        temporary_DF = indexDataFrame_v4(temporary_DF)\n",
    "    \n",
    "    if result_DF_FN:\n",
    "        if '.xlsx' in result_DF_FN:\n",
    "            temporary_DF.to_excel(result_DF_FN)\n",
    "        elif '.csv' in result_DF_FN:\n",
    "            temporary_DF.to_csv(\n",
    "                result_DF_FN, \n",
    "                sep=',', \n",
    "                encoding='utf-8')\n",
    "        \n",
    "    figure = px.imshow(temporary_DF, color_continuous_scale = colorScale)\n",
    "\n",
    "    figure.update_layout(autosize=True)\n",
    "    \n",
    "    if figureTitle:\n",
    "        figure.update_layout(title=figureTitle)\n",
    "    '''\n",
    "    figure.update_layout(\n",
    "        font=dict(\n",
    "            family=\"Arial\",\n",
    "            size=40\n",
    "        )\n",
    "    )'''\n",
    "    \n",
    "    figure.update_layout(coloraxis_showscale=False)\n",
    "    \n",
    "    #figure.show(block=False)\n",
    "    return figure\n",
    "\n",
    "def showFigure_Mtplotlib_v4___(\n",
    "    processingFunction, \n",
    "    useIndex = False,\n",
    "    colorScale=[[0, \"red\"], [0.5 , \"white\"], [1, \"green\"]], \n",
    "    figureTitle=None,\n",
    "    result_DF_FN = None,\n",
    "    colorbarTextPoints = None,\n",
    "    colorbarTexts = None,\n",
    "    colorbarTextRotation = 0):\n",
    "    \n",
    "\n",
    "    global recommender_DF\n",
    "    if recommender_DF.empty:\n",
    "        loadRecommenderDataFrame()\n",
    "    \n",
    "    if not processingFunction:\n",
    "        return\n",
    "    \n",
    "    temporary_DF = pd.DataFrame(index=recommender_DF.index.tolist(), columns=recommender_DF.columns.tolist())\n",
    "    \n",
    "    for rowName in recommender_DF.index:\n",
    "        for columnName in recommender_DF.columns:\n",
    "            temporary_DF.loc[rowName, columnName] = 2 * processingFunction(recommender_DF.loc[rowName, columnName]) - 1  #Map (0, 1) to (-1, 1)\n",
    "    \n",
    "    if useIndex is True:\n",
    "        temporary_DF = indexDataFrame_v4(temporary_DF)\n",
    "    \n",
    "    if result_DF_FN:\n",
    "        if '.xlsx' in result_DF_FN:\n",
    "            temporary_DF.to_excel(result_DF_FN)\n",
    "        elif '.csv' in result_DF_FN:\n",
    "            temporary_DF.to_csv(\n",
    "                result_DF_FN, \n",
    "                sep=',', \n",
    "                encoding='utf-8')\n",
    "\n",
    "    temporary_DF = temporary_DF.astype(float)\n",
    "    \n",
    "    figure = plt.figure(figsize=(15, 15))\n",
    "    figure, ax = plt.subplots(figsize=(15, 15))\n",
    "    \n",
    "    sns.heatmap(\n",
    "        temporary_DF, \n",
    "        cmap=ListedColormap(colorScale), \n",
    "        vmin=-1, \n",
    "        vmax=1, \n",
    "        #linewidths=.5,\n",
    "        cbar=True,\n",
    "        cbar_kws={\n",
    "            'pad': 0.05, \n",
    "            'orientation': 'horizontal',\n",
    "            #'orientation': 'vertical',\n",
    "            \"shrink\": 0.5\n",
    "        },\n",
    "        ax = ax,\n",
    "        square=True, #Square the tiles\n",
    "        )\n",
    "    \n",
    "    #ax.set_aspect('equal') #Square the tiles\n",
    "    \n",
    "    if figureTitle:\n",
    "        plt.title(title=figureTitle)\n",
    "        \n",
    "    plt.yticks(\n",
    "        np.arange(0.5, len(temporary_DF.index), 1), \n",
    "        temporary_DF.index, \n",
    "        rotation = 0)#,fontsize='small')\n",
    "\n",
    "    plt.xticks(\n",
    "        np.arange(0.5, len(temporary_DF.columns), 1),\n",
    "        temporary_DF.columns, \n",
    "        rotation = 90)#, fontsize='small')\n",
    "\n",
    "    #Update colorbar\n",
    "    if colorbarTextPoints and colorbarTexts:\n",
    "        #Get access to colorbar\n",
    "        cbar = ax.collections[0].colorbar\n",
    "        \n",
    "        \n",
    "        #Set font size of colorbar\n",
    "        #cbar.ax.tick_params(labelsize=18)\n",
    "\n",
    "        #Set ticks of colorbar (Texts)\n",
    "        for index, item in enumerate(colorbarTextPoints):\n",
    "            colorbarTextPoints[index] = 2 * colorbarTextPoints[index] - 1\n",
    "        cbar.set_ticks(colorbarTextPoints)\n",
    "        cbar.set_ticklabels(colorbarTexts)\n",
    "        #cbar.ax.set_yticklabels(colorbarTexts, rotation = colorbarTextRotation)  #if 'orientation': 'vertical',\n",
    "        cbar.ax.set_xticklabels(colorbarTexts, rotation = colorbarTextRotation, fontsize='small')  #if 'orientation': 'horizontal',\n",
    "\n",
    "    \n",
    "    plt.setp(ax.get_xticklabels(), horizontalalignment='right', fontsize='small')\n",
    "    \n",
    "    plt.show(block=False)\n",
    "    plt.pause(1)\n",
    "    plt.close()\n",
    "\n",
    "    return figure\n",
    "\n",
    "def showFigure_Mtplotlib_v4_(\n",
    "    processingFunction, \n",
    "    useIndex = False,\n",
    "    colorScale=[[0, \"red\"], [0.5 , \"white\"], [1, \"green\"]], \n",
    "    figureTitle=None,\n",
    "    result_DF_FN = None,\n",
    "    colorbarTextPoints = None,\n",
    "    colorbarTexts = None,\n",
    "    colorbarTextRotation = 0):\n",
    "    \n",
    "\n",
    "    global recommender_DF\n",
    "    if recommender_DF.empty:\n",
    "        loadRecommenderDataFrame()\n",
    "    \n",
    "    if not processingFunction:\n",
    "        return\n",
    "    \n",
    "    temporary_DF = pd.DataFrame(index=recommender_DF.index.tolist(), columns=recommender_DF.columns.tolist())\n",
    "    \n",
    "    for rowName in recommender_DF.index:\n",
    "        for columnName in recommender_DF.columns:\n",
    "            #temporary_DF.loc[rowName, columnName] = 2 * processingFunction(recommender_DF.loc[rowName, columnName]) - 1  #Map (0, 1) to (-1, 1)\n",
    "            temporary_DF.loc[rowName, columnName] = processingFunction(recommender_DF.loc[rowName, columnName])\n",
    "    \n",
    "    if useIndex is True:\n",
    "        temporary_DF = indexDataFrame_v4(temporary_DF)\n",
    "    \n",
    "    if result_DF_FN:\n",
    "        if '.xlsx' in result_DF_FN:\n",
    "            temporary_DF.to_excel(result_DF_FN)\n",
    "        elif '.csv' in result_DF_FN:\n",
    "            temporary_DF.to_csv(\n",
    "                result_DF_FN, \n",
    "                sep=',', \n",
    "                encoding='utf-8')\n",
    "\n",
    "    temporary_DF = temporary_DF.astype(float)\n",
    "    \n",
    "    #temporary_DF.to_csv(os.path.join(outputsFolder, \"showFigure_Mtplotlib_v4_.csv\"))\n",
    "    \n",
    "    #figure = plt.figure(figsize=(20, 20))\n",
    "    #figure, ax = plt.subplots(figsize=(18, 18))\n",
    "    figure, ax = plt.subplots(1, 1, figsize=(12, 18), dpi=60)\n",
    "    \n",
    "    if colorbarTextPoints and colorbarTexts:\n",
    "        cbar = True\n",
    "    else:\n",
    "        cbar = False\n",
    "        \n",
    "    ax = sns.heatmap(\n",
    "        temporary_DF, \n",
    "        cmap=ListedColormap(colorScale), \n",
    "        vmin=0, \n",
    "        vmax=1, \n",
    "        #linewidths=.5,\n",
    "        cbar=cbar,\n",
    "        cbar_kws={\n",
    "            'pad': 0.05, \n",
    "            #'orientation': 'horizontal',\n",
    "            'orientation': 'vertical',\n",
    "            \"shrink\": 0.5\n",
    "        },\n",
    "        ax = ax,\n",
    "        square=True, #Square the tiles\n",
    "        )\n",
    "    \n",
    "    \n",
    "    cmap = ax.collections[0].cmap\n",
    "    cmap.set_under('white')#Colour values less than vmin in white\n",
    "    cmap.set_over('yellow')# colour valued larger than vmax in red\n",
    "    \n",
    "    #ax.set_aspect('equal') #Square the tiles\n",
    "    \n",
    "    if figureTitle:\n",
    "        plt.title(title=figureTitle)\n",
    "        \n",
    "    plt.yticks(\n",
    "        np.arange(0.5, len(temporary_DF.index), 1), \n",
    "        temporary_DF.index, \n",
    "        rotation = 0,\n",
    "        fontsize=22)\n",
    "\n",
    "    plt.xticks(\n",
    "        np.arange(0.5, len(temporary_DF.columns), 1),\n",
    "        temporary_DF.columns, \n",
    "        rotation = 90,\n",
    "        fontsize=22)\n",
    "    \n",
    "    plt.margins(0,0)\n",
    "    \n",
    "    #Update colorbar\n",
    "    if colorbarTextPoints and colorbarTexts:\n",
    "        #Get access to colorbar\n",
    "        cbar = ax.collections[0].colorbar\n",
    "        \n",
    "        #Set font size of colorbar\n",
    "        #cbar.ax.tick_params(labelsize=18)\n",
    "\n",
    "        #Set ticks of colorbar (Texts)\n",
    "        #for index, item in enumerate(colorbarTextPoints):\n",
    "        #    colorbarTextPoints[index] = 2 * colorbarTextPoints[index] - 1\n",
    "        cbar.set_ticks(colorbarTextPoints)\n",
    "        cbar.set_ticklabels(colorbarTexts)\n",
    "        #cbar.ax.set_yticklabels(colorbarTexts, rotation = colorbarTextRotation)  #if 'orientation': 'vertical',\n",
    "        cbar.ax.set_yticklabels(colorbarTexts, rotation = colorbarTextRotation, fontsize=13)  #if 'orientation': 'horizontal',\n",
    "        #cbar.set(size=\"5%\", pad=0.05)\n",
    "        cmap = cbar.cmap\n",
    "        \n",
    "        cmap.set_under('white')#Colour values less than vmin in white\n",
    "        cmap.set_over('yellow')# colour valued larger than vmax in red\n",
    "    \n",
    "    #plt.setp(ax.get_xticklabels(), horizontalalignment='right', fontsize='small')\n",
    "    \n",
    "    plt.show(block=False)\n",
    "    plt.pause(1)\n",
    "    plt.close()\n",
    "\n",
    "    return figure\n",
    "\n",
    "def showFigure_Mtplotlib_v4(\n",
    "    processingFunction, \n",
    "    useIndex = False,\n",
    "    colorScale=[[0, \"red\"], [0.5 , \"white\"], [1, \"green\"]], \n",
    "    figureTitle=None,\n",
    "    result_DF_FN = None,\n",
    "    colorbarTextPoints = None,\n",
    "    colorbarTexts = None,\n",
    "    colorbarTextRotation = 0,\n",
    "    xlabel = None,\n",
    "    ylabel = None):\n",
    "    \n",
    "\n",
    "    global recommender_DF\n",
    "    if recommender_DF.empty:\n",
    "        loadRecommenderDataFrame()\n",
    "    \n",
    "    if not processingFunction:\n",
    "        return\n",
    "    \n",
    "    temporary_DF = pd.DataFrame(index=recommender_DF.index.tolist(), columns=recommender_DF.columns.tolist())\n",
    "    \n",
    "    for rowName in recommender_DF.index:\n",
    "        for columnName in recommender_DF.columns:\n",
    "            #temporary_DF.loc[rowName, columnName] = 2 * processingFunction(recommender_DF.loc[rowName, columnName]) - 1  #Map (0, 1) to (-1, 1)\n",
    "            temporary_DF.loc[rowName, columnName] = processingFunction(recommender_DF.loc[rowName, columnName])\n",
    "    \n",
    "    if useIndex is True:\n",
    "        temporary_DF = indexDataFrame_v4(temporary_DF)\n",
    "    \n",
    "    if result_DF_FN:\n",
    "        if '.xlsx' in result_DF_FN:\n",
    "            temporary_DF.to_excel(result_DF_FN)\n",
    "        elif '.csv' in result_DF_FN:\n",
    "            temporary_DF.to_csv(\n",
    "                result_DF_FN, \n",
    "                sep=',', \n",
    "                encoding='utf-8')\n",
    "\n",
    "    temporary_DF = temporary_DF.astype(float)\n",
    "    #temporary_DF.to_csv(os.path.join(outputsFolder, \"showFigure_Mtplotlib_v4.csv\"))\n",
    "    \n",
    "    \n",
    "    #figure = plt.figure(figsize=(20, 20))\n",
    "    #figure, ax = plt.subplots(figsize=(18, 18))\n",
    "    \n",
    "    #figure, (figureAxis, colorbarAxis) = plt.subplots(1, 2, figsize=(12, 18), dpi=60)\n",
    "    \n",
    "    \n",
    "    #https://stackoverflow.com/questions/13784201/matplotlib-2-subplots-1-colorbar\n",
    "    #figure, figureAxis = plt.subplots(figsize=(12, 18), dpi=60)\n",
    "    figure, figureAxis = plt.subplots(figsize=(14, 18), dpi=60)\n",
    "    figureAxis.set_aspect('auto')\n",
    "    \n",
    "    yticks = temporary_DF.index\n",
    "    xticks = temporary_DF.columns\n",
    "    temporary_DF = temporary_DF.to_numpy()\n",
    "    \n",
    "    img = plt.imshow(temporary_DF, cmap=ListedColormap(colorScale), interpolation='nearest')\n",
    "    \n",
    "    \n",
    "    if figureTitle:\n",
    "        plt.title(figureTitle, fontsize=22)\n",
    "    \n",
    "    if xlabel:\n",
    "        plt.xlabel(xlabel, fontsize=22, labelpad=2)\n",
    "        \n",
    "    if ylabel:\n",
    "        plt.ylabel(ylabel, fontsize=22, labelpad=2)\n",
    "        \n",
    "    plt.yticks(\n",
    "        np.arange(0.5, len(yticks), 1), \n",
    "        yticks, \n",
    "        rotation = 0,\n",
    "        fontsize=12)\n",
    "\n",
    "    plt.xticks(\n",
    "        np.arange(0.5, len(xticks), 1),\n",
    "        xticks, \n",
    "        rotation = 90,\n",
    "        fontsize=12,\n",
    "        horizontalalignment='right')\n",
    "    \n",
    "    figureAxis.tick_params(axis='both', labelsize=22)\n",
    "    \n",
    "    plt.margins(0,0)\n",
    "    \n",
    "    figureAxis.set_aspect('auto')\n",
    "\n",
    "    if colorbarTextPoints and colorbarTexts:\n",
    "        '''\n",
    "        divider = make_axes_locatable(figureAxis)\n",
    "        cax1 = divider.append_axes(\"right\", size=\"5%\", pad=0.5)\n",
    "        colours = img.cmap(img.norm(np.unique(temporary_DF)))\n",
    "        \n",
    "        cbar = figure.colorbar(img, cmap=colours[0:1],  cax=cax1)\n",
    "\n",
    "        colorbarTextPoints = [ round(item, 2) for item in colorbarTextPoints ]\n",
    "        \n",
    "        #cbar = figure.colorbar(img, ax=figureAxis, cax=cbar, orientation='vertical', ticks=colorbarTextPoints)\n",
    "        cbar.set_ticks(colorbarTextPoints)\n",
    "        cbar.set_ticklabels(colorbarTexts)\n",
    "        cbar.ax.tick_params(labelsize=13)\n",
    "        \n",
    "        print(divider.get_position())\n",
    "        '''\n",
    "        \n",
    "        cbar = plt.colorbar(img, pad=0.03, shrink=0.5, ticks=colorbarTextPoints)\n",
    "        cbar.set_ticks(colorbarTextPoints)\n",
    "        #cbar.xaxis.set_ticks(colorbarTextPoints)\n",
    "        cbar.ax.set_yticklabels(colorbarTexts, fontsize=22, rotation = 0)\n",
    "        \n",
    "        #plt.colorbar(shrink=0.5)\n",
    "        \n",
    "    plt.show(block=False)\n",
    "    plt.pause(1)\n",
    "    plt.close()\n",
    "    \n",
    "    return figure\n",
    "\n",
    "def showFigure_Mtplotlib_v4_For_Percentage(\n",
    "    processingFunction, \n",
    "    useIndex = False,\n",
    "    colorScale=[[0, \"red\"], [0.5 , \"white\"], [1, \"green\"]], \n",
    "    figureTitle=None,\n",
    "    result_DF_FN = None,\n",
    "    colorbarTextPoints = None,\n",
    "    colorbarTexts = None,\n",
    "    colorbarTextRotation = 0,\n",
    "    xlabel = None,\n",
    "    ylabel = None):\n",
    "    \n",
    "\n",
    "    global recommender_DF\n",
    "    if recommender_DF.empty:\n",
    "        loadRecommenderDataFrame()\n",
    "    \n",
    "    if not processingFunction:\n",
    "        return\n",
    "    \n",
    "    temporary_DF = pd.DataFrame(index=recommender_DF.index.tolist(), columns=recommender_DF.columns.tolist())\n",
    "    \n",
    "    for rowName in recommender_DF.index:\n",
    "        for columnName in recommender_DF.columns:\n",
    "            temporary_DF.loc[rowName, columnName] = processingFunction(recommender_DF.loc[rowName, columnName])\n",
    "    \n",
    "    if useIndex is True:\n",
    "        temporary_DF = indexDataFrame_v4(temporary_DF)\n",
    "    \n",
    "    if result_DF_FN:\n",
    "        if '.xlsx' in result_DF_FN:\n",
    "            temporary_DF.to_excel(result_DF_FN)\n",
    "        elif '.csv' in result_DF_FN:\n",
    "            temporary_DF.to_csv(\n",
    "                result_DF_FN, \n",
    "                sep=',', \n",
    "                encoding='utf-8')\n",
    "\n",
    "    temporary_DF = temporary_DF.astype(float)\n",
    "\n",
    "    #https://stackoverflow.com/questions/13784201/matplotlib-2-subplots-1-colorbar\n",
    "    figure, figureAxis = plt.subplots(figsize=(14, 18), dpi=60)\n",
    "    figureAxis.set_aspect('auto')\n",
    "    \n",
    "    yticks = temporary_DF.index\n",
    "    xticks = temporary_DF.columns\n",
    "    temporary_DF = temporary_DF.to_numpy()\n",
    "    \n",
    "    img = plt.imshow(temporary_DF, cmap=ListedColormap(colorScale), interpolation='nearest')\n",
    "    \n",
    "    \n",
    "    if figureTitle:\n",
    "        plt.title(figureTitle, fontsize=22)\n",
    "    \n",
    "    if xlabel:\n",
    "        plt.xlabel(xlabel, fontsize=22, labelpad=2)\n",
    "        \n",
    "    if ylabel:\n",
    "        plt.ylabel(ylabel, fontsize=22, labelpad=2)\n",
    "        \n",
    "        \n",
    "    plt.yticks(\n",
    "        np.arange(0.5, len(yticks), 1), \n",
    "        yticks, \n",
    "        rotation = 0,\n",
    "        fontsize=12)\n",
    "\n",
    "    plt.xticks(\n",
    "        np.arange(0.5, len(xticks), 1),\n",
    "        xticks, \n",
    "        rotation = 90,\n",
    "        fontsize=12,\n",
    "        horizontalalignment='right')\n",
    "    \n",
    "    figureAxis.tick_params(axis='both', labelsize=22)\n",
    "    \n",
    "    plt.margins(0,0)\n",
    "    \n",
    "    figureAxis.set_aspect('auto')\n",
    "\n",
    "    if colorbarTextPoints and colorbarTexts:\n",
    "        cbar = plt.colorbar(img, pad=0.03, shrink=0.5, ticks=colorbarTextPoints)\n",
    "        cbar.set_ticks(colorbarTextPoints)\n",
    "        cbar.ax.set_yticklabels(colorbarTexts, fontsize=22, rotation = 0)\n",
    "        \n",
    "    plt.show(block=False)\n",
    "    plt.pause(1)\n",
    "    plt.close()\n",
    "    \n",
    "    return figure\n",
    "\n",
    "\n",
    "def generateReportDataFrame(fileName, columns, rows):\n",
    "\n",
    "    #Get results from executionResult_DF_FN\n",
    "    #confidenceEvaluation_DF = pd.DataFrame(columns=['Pipelines', \"Datasets\", \"Status\", \"AVG_Confidence\"])\n",
    "    reportDataFrame = pd.DataFrame(columns=columns)\n",
    "    \n",
    "    \n",
    "    for row in rows:\n",
    "        reportDataFrame = reportDataFrame.append(\n",
    "            pd.Series(\n",
    "            row, \n",
    "            index=reportDataFrame.columns), ignore_index=True)\n",
    "    \n",
    "    #reportDataFrame.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    reportDataFrame.to_csv(\n",
    "        fileName, \n",
    "        sep = ',', \n",
    "        encoding = 'utf-8',\n",
    "        index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ranking-causing",
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatenateExpertComments_v4():\n",
    "    global recommender_DF,outputsFolder, DATA_TEMPLATE, recommender_DF_FN\n",
    "    \n",
    "    #This block extract Pipeline, Dataset that at least 1 expert commented that are compatible. \n",
    "    listOfFiles = glob.glob(expertsFolder + os.path.sep + '*.csv')\n",
    "\n",
    "    listOfPipelines = []\n",
    "    listOfDatasets = []\n",
    "\n",
    "    success_DS = deepcopy(DATA_TEMPLATE)\n",
    "    success_DS[\"EXPERT_COMMENTS\"] += ['+1']\n",
    "\n",
    "    fail_DS = deepcopy(DATA_TEMPLATE)\n",
    "    fail_DS[\"EXPERT_COMMENTS\"] += ['-1']\n",
    "\n",
    "    recommender_DF = pd.DataFrame()\n",
    "\n",
    "    for inputFile in listOfFiles:\n",
    "\n",
    "        new_DF = pd.read_csv(inputFile, index_col=0)   #Set first column as index\n",
    "\n",
    "        #Adding row and columns to dataFrame\n",
    "        for rowName in new_DF.index:\n",
    "            if rowName.strip() not in listOfPipelines:\n",
    "                listOfPipelines += [rowName.strip()]\n",
    "            for columnName in new_DF.columns:\n",
    "                if columnName.strip() not in listOfDatasets:\n",
    "                    listOfDatasets += [columnName.strip()]\n",
    "    \n",
    "    columnList = [deepcopy(DATA_TEMPLATE) for x in range(len(listOfDatasets))]\n",
    "    zipList = [deepcopy(columnList) for x in range(len(listOfPipelines))]\n",
    "    recommender_DF = pd.DataFrame(zipList, index =listOfPipelines, columns=listOfDatasets)\n",
    "    \n",
    "    #saveRecommenderDataFrame() \n",
    "\n",
    "    for inputFile in listOfFiles:\n",
    "        expertsName = ntpath.basename(inputFile).split('-')[0].strip()\n",
    "        new_DF = pd.read_csv(inputFile, index_col=0)   #Set first column as index\n",
    "\n",
    "        #Adding row and columns to dataFrame\n",
    "        for rowName in new_DF.index:\n",
    "            for columnName in new_DF.columns:\n",
    "                rowName = rowName.strip()\n",
    "                columnName = columnName.strip()\n",
    "                \n",
    "                recommender_DF.loc[rowName,columnName][\"NUM_OF_EXPERT_COMMENTS\"] += 1\n",
    "                if new_DF.loc[rowName,columnName] in ['x', 'X']:\n",
    "                    recommender_DF.loc[rowName,columnName][\"EXPERT_COMMENTS\"] += ['+1']\n",
    "                    recommender_DF.loc[rowName,columnName][\"EXPERTS_COMMENTED_SUCCESS\"] += [expertsName]\n",
    "                else:\n",
    "                    recommender_DF.loc[rowName,columnName][\"EXPERT_COMMENTS\"] += ['-1']\n",
    "    \n",
    "    saveRecommenderDataFrame()   \n",
    "        \n",
    "    print(\"concatenateExpertComments_v4 Done\")\n",
    "    \n",
    "#concatenateExpertComments_v4()     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "instrumental-carnival",
   "metadata": {},
   "outputs": [],
   "source": [
    "def addExpertsConfidence_v4():\n",
    "    global recommender_DF, surveyInputFile, recommender_DF_FN\n",
    "    \n",
    "    if not os.path.exists(surveyInputFile):\n",
    "        print(\"Input file not found!\")\n",
    "    \n",
    "    loadRecommenderDataFrame()\n",
    "    \n",
    "    #Get list of pipelines and keys\n",
    "    pipelinesList = []\n",
    "    datasetsList = []\n",
    "    trimmedPipelinesList = []\n",
    "    trimmedDatasetsList = []\n",
    "\n",
    "    csvFile = open(surveyInputFile,\"r\") \n",
    "    titlesLine = csvFile.readline().split(',')\n",
    "    for item in titlesLine:\n",
    "        if 'How well do you know the data processing pipelines below? [' in item:\n",
    "            trimmedPipelinesList += [item.replace('\\n', '').replace('How well do you know the data processing pipelines below? [', '').replace(']', '').split('(http')[0].split('http')[0]]\n",
    "            pipelinesList += [item.replace('\\n', '')]\n",
    "        if 'How well do you know the datasets below? [' in item:\n",
    "            trimmedDatasetsList += [item.replace('\\n', '').replace('How well do you know the datasets below? [', '').replace(']', '').split(' http')[0]]\n",
    "            datasetsList += [item.replace('\\n', '')]\n",
    "\n",
    "    #Get whole content as Table\n",
    "    confidenceTable = np.zeros([len(pipelinesList), len(datasetsList)])\n",
    "\n",
    "    contentTable = None\n",
    "    with open(surveyInputFile, newline='') as csvfile:\n",
    "        contentTable = csv.DictReader(csvfile)\n",
    "        for row in contentTable:\n",
    "            expertsName = row['Name']\n",
    "            for rowIndex, pipeline in enumerate(pipelinesList):\n",
    "                for columnIndex, dataset in enumerate(datasetsList):\n",
    "                    pipelineName = trimmedPipelinesList[rowIndex].strip()\n",
    "                    datasetName = trimmedDatasetsList[columnIndex].strip()\n",
    "                    if (pipelineName in recommender_DF.index.tolist()) and (datasetName in recommender_DF.columns.tolist()):\n",
    "                        if expertsName in recommender_DF.loc[pipelineName, datasetName][\"EXPERTS_COMMENTED_SUCCESS\"]:\n",
    "                            #if row[pipeline] == 'Some idea':\n",
    "                            #    print(\"Pipeline :\", expertsName)\n",
    "                            #if row[dataset] == 'Some idea':\n",
    "                            #    print(\"Dataset :\", expertsName)\n",
    "                            if row[pipeline]:\n",
    "                                    recommender_DF.loc[pipelineName, datasetName][\"EXPERT_CONFIDENCE_LIST\"] += [row[pipeline]]\n",
    "                            if row[dataset]:\n",
    "                                if expertsName in ['Expert 1', 'Expert 2']:\n",
    "                                    if row[dataset].lower() == 'some idea':\n",
    "                                        recommender_DF.loc[pipelineName, datasetName][\"EXPERT_CONFIDENCE_LIST\"] += ['Good knowledge']\n",
    "                                    else:\n",
    "                                        recommender_DF.loc[pipelineName, datasetName][\"EXPERT_CONFIDENCE_LIST\"] += [row[dataset]]\n",
    "                                else:\n",
    "                                    recommender_DF.loc[pipelineName, datasetName][\"EXPERT_CONFIDENCE_LIST\"] += [row[dataset]]\n",
    "                            \n",
    "    '''\n",
    "    with open(surveyInputFile, newline='') as csvfile:\n",
    "        contentTable = csv.DictReader(csvfile)\n",
    "        for row in contentTable:\n",
    "            for rowIndex, pipeline in enumerate(pipelinesList):\n",
    "                for columnIndex, dataset in enumerate(datasetsList):\n",
    "                    #print()\n",
    "                    if (row[pipeline].lower() in ['expert', 'good knowledge']) and \\\n",
    "                        (row[dataset].lower() in ['some idea']):\n",
    "                        print(\"Found : \", row['Name'], \" @ \", trimmedPipelinesList[rowIndex], \" ****** \", trimmedDatasetsList[columnIndex])\n",
    "    '''                    \n",
    "    fileHandle = open(asessedPipelinesFile, \"w\")\n",
    "    for pipeline in trimmedPipelinesList:\n",
    "        fileHandle.write(pipeline + '\\n')\n",
    "    fileHandle.close()\n",
    "    \n",
    "    fileHandle = open(assessedDatasetFile, \"w\")\n",
    "    for dataset in trimmedDatasetsList:\n",
    "        fileHandle.write(dataset + '\\n')\n",
    "    fileHandle.close()\n",
    "    \n",
    "    saveRecommenderDataFrame()   \n",
    "    \n",
    "    print(\"addExpertsConfidence_v4 Done\")\n",
    "    \n",
    "#addExpertsConfidence_v4()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "vital-rachel",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trimDataframe_v4():\n",
    "    global recommender_DF\n",
    "    loadRecommenderDataFrame()\n",
    "    fileContentStringList = []\n",
    "    \n",
    "    indexesOfEmptyRows = recommender_DF.index.tolist()\n",
    "    for rowName in recommender_DF.index.tolist():  #Pipeline name\n",
    "            \n",
    "        for rowItem in recommender_DF.loc[rowName]:\n",
    "            if \"+1\" in rowItem[\"EXPERT_COMMENTS\"]:\n",
    "                if rowName in indexesOfEmptyRows:\n",
    "                    indexesOfEmptyRows.remove(rowName)\n",
    "            \n",
    "    fileContentStringList += [\"Dropped Pipelines:\"]\n",
    "    fileContentStringList += [indexesOfEmptyRows]\n",
    "    \n",
    "    #print(\"Removed Pipelines:\")\n",
    "    #print(indexesOfEmptyRows, '\\n\\n')\n",
    "    \n",
    "    if indexesOfEmptyRows:\n",
    "        recommender_DF = recommender_DF.drop(indexesOfEmptyRows)\n",
    "    \n",
    "    nameOfEmptyColumns = recommender_DF.columns.tolist()\n",
    "    for columnName in recommender_DF.columns.tolist():  #Dataset name\n",
    "        for rowName in recommender_DF.index.tolist():  #Pipeline name\n",
    "            if \"+1\" in recommender_DF.loc[rowName, columnName][\"EXPERT_COMMENTS\"]: \n",
    "                if columnName in nameOfEmptyColumns:\n",
    "                    nameOfEmptyColumns.remove(columnName)\n",
    "\n",
    "    #print(\"Removed Datasets:\")\n",
    "    #print(nameOfEmptyColumns, '\\n\\n')\n",
    "\n",
    "    fileContentStringList += [\"Dropped Datasets:\"]\n",
    "    fileContentStringList += [nameOfEmptyColumns]\n",
    "    saveReportFile(\"trimDataframe_v4\", fileContentStringList)\n",
    "    \n",
    "    if nameOfEmptyColumns:\n",
    "        recommender_DF = recommender_DF.drop(columns=nameOfEmptyColumns)\n",
    "    \n",
    "    saveRecommenderDataFrame() \n",
    "    print(\"trimDataframe_v4 Done\")\n",
    "    \n",
    "#trimDataframe_v4()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "legal-attention",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractIndexTables_v4():\n",
    "    global recommender_DF, confidenceRatings\n",
    "    loadRecommenderDataFrame()\n",
    "    \n",
    "    #create the Abbreviation  for pipeline names\n",
    "    pipelineAbbreviationDictionary = dict()\n",
    "    pipelineList = recommender_DF.index.tolist()\n",
    "    df = pd.DataFrame(columns=['Pipeline Name', 'Index'])  #Table dataframe\n",
    "    for index, item in enumerate(pipelineList):\n",
    "        pipelineAbbreviationDictionary[item] = \"P\" + str(index)\n",
    "        df.loc[item] = [item, \"P\" + str(index)]\n",
    "    df = df.set_index('Pipeline Name')\n",
    "    df.to_excel(pipelineIndexTable_DF_FN)\n",
    "    \n",
    "    #create the Abbreviation  for dataset names\n",
    "    datasetAbbreviationDictionary = dict()\n",
    "    datasetList = recommender_DF.columns.tolist()\n",
    "    df = pd.DataFrame(columns=['Dataset Name', 'Index'])  #Table dataframe\n",
    "    for index, item in enumerate(datasetList):\n",
    "        datasetAbbreviationDictionary[item] = \"D\" + str(index)\n",
    "        df.loc[item] = [item, \"D\" + str(index)]\n",
    "    df = df.set_index('Dataset Name')\n",
    "    df.to_excel(datasetIndexTable_DF_FN)\n",
    "    \n",
    "    print(\"extractIndexTables_v4 Done\")\n",
    "\n",
    "#extractIndexTables_v4()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "western-killer",
   "metadata": {},
   "outputs": [],
   "source": [
    "def addConfidenceRatings_v4():\n",
    "    global recommender_DF, confidenceRatings\n",
    "    loadRecommenderDataFrame()\n",
    "    \n",
    "    for rowName in recommender_DF.index:\n",
    "        for columnName in recommender_DF.columns:\n",
    "            if len(recommender_DF.loc[rowName, columnName][\"EXPERT_CONFIDENCE_LIST\"]) > 0:\n",
    "                averageConfidence = 0\n",
    "                for confidence in recommender_DF.loc[rowName, columnName][\"EXPERT_CONFIDENCE_LIST\"]:\n",
    "                    averageConfidence += confidenceRatings[confidence]\n",
    "                averageConfidence /= len(recommender_DF.loc[rowName, columnName][\"EXPERT_CONFIDENCE_LIST\"])\n",
    "                recommender_DF.loc[rowName, columnName][\"CONFIDENCE_RATING\"] = averageConfidence\n",
    "    \n",
    "    saveRecommenderDataFrame() \n",
    "    print(\"addConfidenceRatings_v4 Done\")\n",
    "\n",
    "#addConfidenceRatings_v4()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "sorted-suspect",
   "metadata": {},
   "outputs": [],
   "source": [
    "def addExecutionResult_v4():\n",
    "    \n",
    "    global recommender_DF\n",
    "    loadRecommenderDataFrame()\n",
    "\n",
    "    #Get results from executionResult_DF_FN\n",
    "    executionResult_DF = pd.read_excel(executionResult_DF_FN)\n",
    "    executionResult_DF = executionResult_DF.set_index(executionResult_DF.columns[0])\n",
    "\n",
    "    for rowName in recommender_DF.index:\n",
    "        for columnName in recommender_DF.columns:\n",
    "            for rowIndex in range(len(executionResult_DF.index)):\n",
    "                executionResult = executionResult_DF['Reason'][rowIndex]\n",
    "                if executionResult is numpy.nan:\n",
    "                    executionResult = 'success'\n",
    "                    \n",
    "                if (rowName.strip() == executionResult_DF['Pipelines'][rowIndex].strip()) and (columnName.strip() == executionResult_DF['Datasets'][rowIndex].strip()):\n",
    "                    recommender_DF.loc[rowName,columnName][\"EXECUTION_RESULT_DETAILS\"] = executionResult\n",
    "\n",
    "    \n",
    "    saveRecommenderDataFrame()\n",
    "    print(\"addExecutionResult_v4 Done\")\n",
    "\n",
    "#addExecutionResult_v4()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "honey-december",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateConfidenceEvaluationReport_v4():\n",
    "    \n",
    "    global recommender_DF\n",
    "    loadRecommenderDataFrame()\n",
    "\n",
    "    ##Get results from executionResult_DF_FN\n",
    "    #confidenceEvaluation_DF = pd.DataFrame(columns=['Pipelines', \"Datasets\", \"Status\", \"AVG_Confidence\"])\n",
    "    \n",
    "    failedReasons = [\n",
    "        'failed', \n",
    "        'wrong data', \n",
    "        'wrong format', \n",
    "        'not available', \n",
    "        'pipeline issue']\n",
    "     \n",
    "    rows = []\n",
    "    \n",
    "    count = 0\n",
    "    for rowName in recommender_DF.index:\n",
    "        for columnName in recommender_DF.columns:\n",
    "            #print(rowName, columnName, recommender_DF.loc[rowName, columnName]['EXECUTION_RESULT_DETAILS'] )\n",
    "            if recommender_DF.loc[rowName, columnName]['EXECUTION_RESULT_DETAILS'] in failedReasons:\n",
    "                recommender_DF.loc[rowName, columnName]['EXECUTION_RESULT_BRIEF'] = 'failed'\n",
    "                count += 1\n",
    "\n",
    "                newRow = [\\\n",
    "                    rowName,\n",
    "                    columnName, \n",
    "                    'failed',\n",
    "                    recommender_DF.loc[rowName, columnName]['CONFIDENCE_RATING']]\n",
    "                \n",
    "                #confidenceEvaluation_DF = confidenceEvaluation_DF.append(\n",
    "                #    pd.Series(\n",
    "                #    newRow, \n",
    "                #    index=confidenceEvaluation_DF.columns), ignore_index=True)\n",
    "                rows += [newRow]\n",
    "                \n",
    "            elif recommender_DF.loc[rowName, columnName]['EXECUTION_RESULT_DETAILS'] in ['success']:\n",
    "                recommender_DF.loc[rowName, columnName]['EXECUTION_RESULT_BRIEF'] = 'success'\n",
    "                count += 1\n",
    "\n",
    "                newRow = [\\\n",
    "                    rowName,\n",
    "                    columnName, \n",
    "                    'success',\n",
    "                    recommender_DF.loc[rowName, columnName]['CONFIDENCE_RATING']]\n",
    "                \n",
    "                #confidenceEvaluation_DF = confidenceEvaluation_DF.append(\n",
    "                #    pd.Series(\n",
    "                #    newRow, \n",
    "                #    index=confidenceEvaluation_DF.columns), ignore_index=True)\n",
    "                rows += [newRow]\n",
    "                \n",
    "    print(\"Number of Confidence Evaluation Pipeline-Datasets :\", count)\n",
    "    \n",
    "    ##confidenceEvaluation_DF.set_index('Pipelines')\n",
    "    #confidenceEvaluation_DF.index.name = 'Pipelines'\n",
    "    #confidenceEvaluation_DF.to_csv(\n",
    "    #    confidenceEvaluationReport_DF_FN, \n",
    "    #    sep=',', \n",
    "    #    encoding='utf-8')\n",
    "    \n",
    "    generateReportDataFrame(\n",
    "        confidenceEvaluationReport_DF_FN,\n",
    "        columns = ['Pipelines', \"Datasets\", \"Status\", \"Average expert confidence\"],\n",
    "        rows = rows\n",
    "        )\n",
    "    \n",
    "    saveRecommenderDataFrame()\n",
    "    print(\"generateConfidenceEvaluationReport_v4 Done\")\n",
    "\n",
    "#generateConfidenceEvaluationReport_v4()      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "harmful-inside",
   "metadata": {},
   "outputs": [],
   "source": [
    "def addFractionRates_v4():\n",
    "    global recommender_DF\n",
    "    loadRecommenderDataFrame()\n",
    "\n",
    "    pipelinesIndexDict = getPipelinesIndexDict()\n",
    "    datasetsIndexDict = getDatasetsIndexDict()\n",
    "    \n",
    "    rows = []\n",
    "    \n",
    "    for rowName in recommender_DF.index:\n",
    "        for columnName in recommender_DF.columns:\n",
    "            if recommender_DF.loc[rowName, columnName]['EXECUTION_RESULT_BRIEF']:\n",
    "                recommender_DF.loc[rowName, columnName]['FRACTION_RATE_SUCCESS_OVER_ALL'] = \\\n",
    "                    round(recommender_DF.loc[rowName, columnName]['EXPERT_COMMENTS'].count(\"+1\") / 13, 2)\n",
    "\n",
    "                if recommender_DF.loc[rowName, columnName]['EXPERT_COMMENTS']:\n",
    "                    recommender_DF.loc[rowName, columnName]['FRACTION_RATE_SUCCESS_OVER_COMMENTED'] = \\\n",
    "                        round (recommender_DF.loc[rowName, columnName]['EXPERT_COMMENTS'].count(\"+1\")  / \\\n",
    "                        len(recommender_DF.loc[rowName, columnName]['EXPERT_COMMENTS']), 2)\n",
    "                else:\n",
    "                    recommender_DF.loc[rowName, columnName]['FRACTION_RATE_SUCCESS_OVER_COMMENTED'] = 0\n",
    "                \n",
    "                newRow = [\\\n",
    "                        rowName,\n",
    "                        columnName,\n",
    "                        recommender_DF.loc[rowName, columnName]['EXECUTION_RESULT_BRIEF'],\n",
    "                        recommender_DF.loc[rowName, columnName]['FRACTION_RATE_SUCCESS_OVER_COMMENTED']]\n",
    "                rows += [newRow]\n",
    "                \n",
    "    generateReportDataFrame(\n",
    "        fractionRateReport_DF_FN,\n",
    "        columns = ['Pipelines', \"Datasets\", \"Status\", \"Fraction of experts\"],\n",
    "        rows = rows\n",
    "        )\n",
    "    \n",
    "    saveRecommenderDataFrame()\n",
    "    print(\"addFractionRates_v4 Done\")\n",
    "\n",
    "#addFractionRates_v4()          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "durable-haiti",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateAlsModelInput_v4():\n",
    "    global recommender_DF\n",
    "    loadRecommenderDataFrame()\n",
    "\n",
    "    pipelinesIndexDict = getPipelinesIndexDict()\n",
    "    datasetsIndexDict = getDatasetsIndexDict()\n",
    "    \n",
    "    rows = []\n",
    "    \n",
    "    for rowName in recommender_DF.index:\n",
    "        for columnName in recommender_DF.columns:\n",
    "            if recommender_DF.loc[rowName, columnName]['EXECUTION_RESULT_BRIEF']:\n",
    "                result = 1\n",
    "                if (recommender_DF.loc[rowName, columnName]['EXECUTION_RESULT_BRIEF'] == 'success'):\n",
    "                    result = 2\n",
    "                newRow = [\\\n",
    "                        pipelinesIndexDict[rowName].replace(\"P\", \"\"),\n",
    "                        datasetsIndexDict[columnName].replace(\"D\", \"\"),\n",
    "                        result]\n",
    "                rows += [newRow]\n",
    "                \n",
    "    generateReportDataFrame(\n",
    "        alsModelInput_DF_FN,\n",
    "        columns = ['Pipelines', \"Datasets\", \"Status\"],\n",
    "        rows = rows\n",
    "        )\n",
    "    \n",
    "    saveRecommenderDataFrame()\n",
    "    print(\"generateAlsModelInput_v4 Done\")\n",
    "\n",
    "#generateAlsModelInput_v4()   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "desirable-london",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetchPipelinesInfo_v4():\n",
    "    ############################vvvvvvvvvvvvvvvvvvvvvv This section is from : /conp-portal/app/threads.py \n",
    "    searcher = Searcher(query=None, max_results=100, no_trunc=True, verbose=True)\n",
    "    all_descriptors = searcher.search()\n",
    "    # then pull every single descriptor\n",
    "    all_descriptor_ids = list(map(lambda x: x[\"ID\"], all_descriptors))\n",
    "    files = Puller(all_descriptor_ids).pull()\n",
    "\n",
    "    # fetch every single descriptor into one file\n",
    "    detailed_all_descriptors = list(map(lambda f: json.load(open(f, 'r')), files))\n",
    "    \n",
    "    # store data in cache\n",
    "    with open(pipelineDescriptors_FN, \"w\") as f:\n",
    "        json.dump(all_descriptors, f, indent=4)\n",
    "\n",
    "    #with open(detailed_all_descriptors_File, \"w\") as f:\n",
    "    #    json.dump(detailed_all_descriptors, f, indent=4)\n",
    "    ############################^^^^^^^^^^^^^^^^^^^^^^ This section is from : /conp-portal/app/threads.py \n",
    "    print(\"fetchPipelinesInfor_v4 Done\")\n",
    "    \n",
    "    \n",
    "def fetchDatasetsInfo_v4():\n",
    "    #Get information about datasets\n",
    "    RECOMMENDER_V4_DATASETS_PATH = os.path.join(os.getenv(\"HOME\"), \".cache/recommender-v4-datasets\")\n",
    "    if not os.path.exists(RECOMMENDER_V4_DATASETS_PATH):\n",
    "        os.mkdir(RECOMMENDER_V4_DATASETS_PATH)\n",
    "\n",
    "    repository = git.Git(RECOMMENDER_V4_DATASETS_PATH)\n",
    "    if not os.path.exists(RECOMMENDER_V4_DATASETS_PATH):\n",
    "        repository.clone(\"https://github.com/CONP-PCNO/conp-dataset.git\", recursive=True, branch='master', progress=True)\n",
    "    #else:\n",
    "    #    repository.checkout('master') #.fetch(\"https://github.com/CONP-PCNO/conp-dataset.git\", recursive=True, branch='master', progress=True)\n",
    "    #    repository.fetch()\n",
    "    print(\"fetchDatasetsInfo_v4 Done\")\n",
    "\n",
    "\n",
    "def getPipelinesAndDatasetsStatisctics_v4():\n",
    "\n",
    "    \n",
    "    with open(asessedPipelinesFile) as f:\n",
    "        asessedPipelines = list()\n",
    "        for item in list(f.read().split('\\n')):\n",
    "            if item.strip():\n",
    "                asessedPipelines += [item.strip()]\n",
    "\n",
    "    with open(pipelineDescriptors_FN) as f:\n",
    "        all_descriptors = json.load(f)\n",
    "    \n",
    "    #Get statistics for TAGS\n",
    "    pipelineDomains = []\n",
    "    for pipelineDescriptor in all_descriptors:\n",
    "        if pipelineDescriptor[\"TITLE\"] in asessedPipelines:\n",
    "            for tag in pipelineDescriptor['TAGS'].split(','):\n",
    "                if 'domain' in tag:\n",
    "                    tagName = tag.split(':')[1]\n",
    "                    if tagName not in pipelineDomains:\n",
    "                        pipelineDomains += [tagName]\n",
    "    \n",
    "    #Stores statisctics\n",
    "    statistic = {\n",
    "        \"Pipelines TAGS\": {}, \n",
    "        \"Pipelines CONTAINER\":{\"docker\" : 0, \"singularity\": 0}, \n",
    "        \"Datasets Format\": {}, \n",
    "        \"Datasets Modality\": {}, \n",
    "        \"Datasets Keyword\": {}, \n",
    "        \"Datasets File Size\": [],\n",
    "        \"Datasets Total Size (GB)\": 0,\n",
    "        \"Datasets Num Of Files\": 0,\n",
    "        \"Datasets Subjects\": 0,\n",
    "        \n",
    "    }\n",
    "    \n",
    "    for pipelineDomain in pipelineDomains:\n",
    "        statistic[\"Pipelines TAGS\"][pipelineDomain] = 0\n",
    "        \n",
    "    for pipelineTitle in asessedPipelines:\n",
    "        for pipelineDescriptor in all_descriptors:\n",
    "            if pipelineDescriptor[\"ID\"] != \"zenodo.3879740\":  #there are 2 \"TITLE\": \"BEst\"\n",
    "                if pipelineDescriptor[\"TITLE\"] == pipelineTitle:\n",
    "                    for pipelineDomain in pipelineDomains:\n",
    "                        if pipelineDomain in pipelineDescriptor['TAGS']:\n",
    "                            statistic[\"Pipelines TAGS\"][pipelineDomain] += 1\n",
    "\n",
    "                    #Get statistics for CONTAINER\n",
    "                    if \"docker\" in pipelineDescriptor[\"CONTAINER\"]:\n",
    "                        statistic[\"Pipelines CONTAINER\"][\"docker\"] += 1\n",
    "                    if \"singularity\" in pipelineDescriptor[\"CONTAINER\"]:\n",
    "                        statistic[\"Pipelines CONTAINER\"][\"singularity\"] += 1\n",
    "            \n",
    "    \n",
    "    RECOMMENDER_V4_DATASETS_PATH = os.path.join(os.getenv(\"HOME\"), \".cache/recommender-v4-datasets\")\n",
    "\n",
    "    crowlerFolderAddressList = os.listdir(os.path.join(RECOMMENDER_V4_DATASETS_PATH, \"conp-dataset\", \"projects\"))\n",
    "    for index, item in enumerate(crowlerFolderAddressList):\n",
    "        crowlerFolderAddressList[index] = os.path.join(RECOMMENDER_V4_DATASETS_PATH, \"conp-dataset\", \"projects\", item)\n",
    "\n",
    "    datasetDescriptorFileAddress = []\n",
    "    for folder in crowlerFolderAddressList:\n",
    "        if os.path.isdir(folder):\n",
    "            if os.path.exists(os.path.join(folder, \"DATS.json\")):\n",
    "                datasetDescriptorFileAddress += [os.path.join(RECOMMENDER_V4_DATASETS_PATH, folder, \"DATS.json\")]\n",
    "            else:\n",
    "                subfolderList = os.listdir(os.path.join(RECOMMENDER_V4_DATASETS_PATH, folder))\n",
    "                for index, item in enumerate(subfolderList):\n",
    "                    subfolderList[index] = os.path.join(RECOMMENDER_V4_DATASETS_PATH, folder, item)\n",
    "                crowlerFolderAddressList += subfolderList\n",
    "\n",
    "    allDatasetDescriptors = []\n",
    "    for datasetDescriptor in datasetDescriptorFileAddress:\n",
    "        with open(datasetDescriptor) as f:\n",
    "            allDatasetDescriptors += [json.load(f)]\n",
    "    \n",
    "    datasetFormats = []\n",
    "    datasetModalities = []\n",
    "    datasetKeywords = []\n",
    "    for datasetDescriptor in allDatasetDescriptors:\n",
    "        if \"formats\" in datasetDescriptor[\"distributions\"][0].keys():\n",
    "            for datasetFormat in datasetDescriptor[\"distributions\"][0][\"formats\"]:\n",
    "                if datasetFormat not in datasetFormats:\n",
    "                    datasetFormats += [datasetFormat]\n",
    "    \n",
    "    for datasetDescriptor in allDatasetDescriptors:\n",
    "        for t in datasetDescriptor.get('types', []):\n",
    "            info = t.get('information', {})\n",
    "            modality = info.get('value', None)\n",
    "            if (modality is not None) and (modality.lower() not in datasetModalities):\n",
    "                datasetModalities.append(modality.lower())\n",
    "        for t in datasetDescriptor.get('keywords', []):\n",
    "            keyword = t.get('value', None)\n",
    "            if (keyword is not None) and (keyword not in datasetKeywords):\n",
    "                datasetKeywords.append(keyword.lower())\n",
    "        \n",
    "    for datasetFormat in datasetFormats:\n",
    "        statistic[\"Datasets Format\"][datasetFormat] = 0\n",
    "    for datasetModality in datasetModalities:\n",
    "        statistic[\"Datasets Modality\"][datasetModality] = 0\n",
    "    for datasetKeyword in datasetKeywords:\n",
    "        statistic[\"Datasets Keyword\"][datasetKeyword] = 0\n",
    "\n",
    "    with open(assessedDatasetFile) as f:\n",
    "        assessedDatasets = f.readlines()\n",
    "    \n",
    "    statistic[\"Datasets Total Size (GB)\"] = 0\n",
    "    for datasetTitle in assessedDatasets:\n",
    "        for datasetDescriptorFile in datasetDescriptorFileAddress:\n",
    "            with open(datasetDescriptorFile) as f:\n",
    "                datasetDescriptor = json.load(f)\n",
    "                \n",
    "                if len(datasetTitle.strip().split(os.path.sep)) == 1:\n",
    "                    datasetTitle = datasetTitle.strip()\n",
    "                else:\n",
    "                    datasetTitle = datasetTitle.strip().split(os.path.sep)[1]\n",
    "                \n",
    "                if datasetTitle in datasetDescriptorFile.split(os.path.sep):\n",
    "                    if \"formats\" in datasetDescriptor[\"distributions\"][0].keys():\n",
    "                        for datasetFormat in datasetDescriptor[\"distributions\"][0][\"formats\"]:\n",
    "                            statistic[\"Datasets Format\"][datasetFormat] += 1\n",
    "\n",
    "                    for t in datasetDescriptor.get('types', []):\n",
    "                        info = t.get('information', {})\n",
    "                        modality = info.get('value', None)\n",
    "                        if (modality is not None):\n",
    "                            statistic[\"Datasets Modality\"][modality.lower()] += 1\n",
    "                    \n",
    "                    for t in datasetDescriptor.get('keywords', []):\n",
    "                        keyword = t.get('value', {})\n",
    "                        if (keyword is not None):\n",
    "                            statistic[\"Datasets Keyword\"][keyword.lower()] += 1\n",
    "    \n",
    "    \n",
    "                    dists = datasetDescriptor.get('distributions', None)\n",
    "                    if dists:\n",
    "                        if not type(dists) == list:\n",
    "                            if dists.get('@type', '') == 'DatasetDistribution':\n",
    "                                dist = dists\n",
    "                            else:\n",
    "                                dist = {}\n",
    "                        else:\n",
    "                            # Taking the first distribution size. (arbitrary choice)\n",
    "                            dist = dists[0]\n",
    "\n",
    "                        size = float(dist.get('size', 0))\n",
    "                        unit = dist.get('unit', {}).get('value', '')\n",
    "\n",
    "                        # Some data values from the DATS are not user friendly so\n",
    "                        # If size > 1000, divide n times until it is < 1000 and increment the units from the array\n",
    "                        units = ['B', 'KB', 'MB', 'GB', 'TB', 'PB', 'EB']\n",
    "                        count = 0\n",
    "\n",
    "                        while size > 1000:\n",
    "                            size /= 1000\n",
    "                            count += 1\n",
    "\n",
    "                        size = round(size, 1)\n",
    "                        unit = units[units.index(unit) + count]\n",
    "                        statistic[\"Datasets File Size\"] += [\"{} {}\".format(size, unit)]\n",
    "                        \n",
    "                        if unit == 'B':\n",
    "                            statistic[\"Datasets Total Size (GB)\"] += size / 1000000000\n",
    "                        elif unit == 'KB':\n",
    "                            statistic[\"Datasets Total Size (GB)\"] += size / 1000000\n",
    "                        elif unit == 'MB':\n",
    "                            statistic[\"Datasets Total Size (GB)\"] += size / 1000\n",
    "                        elif unit == 'GB':\n",
    "                            statistic[\"Datasets Total Size (GB)\"] += size\n",
    "                        elif unit == 'TB':\n",
    "                            statistic[\"Datasets Total Size (GB)\"] += size * 1000\n",
    "                        elif unit == 'PB':\n",
    "                            statistic[\"Datasets Total Size (GB)\"] += size * 1000000\n",
    "                        elif unit == 'EB':\n",
    "                            statistic[\"Datasets Total Size (GB)\"] += size * 1000000000\n",
    "\n",
    "                    count = 0\n",
    "                    extraprops = datasetDescriptor.get('extraProperties', {})\n",
    "                    for prop in extraprops:\n",
    "                        if prop.get('category') == 'files':\n",
    "                            for x in prop.get('values', []):\n",
    "                                if isinstance(x['value'], str):\n",
    "                                    count += int(x['value'].replace(\",\", \"\"))\n",
    "                                else:\n",
    "                                    count += x['value']\n",
    "                    statistic[\"Datasets Num Of Files\"] += count\n",
    "\n",
    "                    count = 0\n",
    "                    extraprops = datasetDescriptor.get('extraProperties', {})\n",
    "                    for prop in extraprops:\n",
    "                        if prop.get('category') == 'subjects':\n",
    "                            for x in prop.get('values', []):\n",
    "                                if isinstance(x['value'], str):\n",
    "                                    count += int(x['value'].replace(\",\", \"\"))\n",
    "                                else:\n",
    "                                    count += x['value']\n",
    "                    statistic[\"Datasets Subjects\"] += count\n",
    "     \n",
    "    #remove \"\" from datasets keywords\n",
    "    if 'canadian-open-neuroscience-platform' in statistic[\"Datasets Keyword\"].keys():\n",
    "        del statistic[\"Datasets Keyword\"]['canadian-open-neuroscience-platform']\n",
    "    \n",
    "    \n",
    "    #Pipelines TAGS\n",
    "    for item in list(statistic[\"Pipelines TAGS\"]):\n",
    "        if statistic[\"Pipelines TAGS\"][item] == 0:\n",
    "            statistic[\"Pipelines TAGS\"].pop(item)\n",
    "            \n",
    "    sortedStatistic = {k: v for k, v in sorted(statistic[\"Pipelines TAGS\"].items(), key=lambda item: item[1], reverse=False)}\n",
    "    \n",
    "    y_pos = np.arange(len(sortedStatistic.keys()))\n",
    "    plt.subplots(figsize=(5,8))\n",
    "    plt.barh(y_pos, sortedStatistic.values(), align='center', alpha=1, color='red') \n",
    "    plt.margins(y=0.01)\n",
    "    plt.yticks(y_pos, sortedStatistic.keys(), fontsize=18) \n",
    "    plt.xticks(fontsize=18)\n",
    "    plt.xlabel('Number of Pipelines', fontsize=18)\n",
    "    #plt.ylabel('Pipelines Tag', fontsize=18)\n",
    "    #plt.savefig(os.path.join(figuresFolder, 'Pipelines Tag.jpg'), bbox_inches=\"tight\")\n",
    "    plt.savefig(os.path.join(figuresFolder, 'Pipelines Tag.pdf'), bbox_inches=\"tight\")\n",
    "    plt.show(block=False)\n",
    "    plt.pause(1)\n",
    "    plt.close()\n",
    "    \n",
    "    #Datasets Type\n",
    "    for item in list(statistic[\"Datasets Keyword\"]):\n",
    "        if statistic[\"Datasets Keyword\"][item] == 0:\n",
    "            statistic[\"Datasets Keyword\"].pop(item)\n",
    "    \n",
    "    sortedStatistic = {k: v for k, v in sorted(statistic[\"Datasets Keyword\"].items(), key=lambda item: item[1], reverse=False)}\n",
    "    \n",
    "    y_pos = np.arange(len(sortedStatistic.values()))\n",
    "    plt.subplots(figsize=(5,30))\n",
    "    plt.barh(y_pos, sortedStatistic.values(), align='center', alpha=1, color='blue') \n",
    "    plt.margins(y=0.01)\n",
    "    plt.yticks(y_pos, sortedStatistic.keys(), fontsize=18)  \n",
    "    xValues = [*range(0, max(sortedStatistic.values()) + 1, 1)]\n",
    "    plt.xticks(xValues, xValues, fontsize=18)\n",
    "    plt.xlabel('Number of Datasets', fontsize=18)\n",
    "    #plt.ylabel('Dataset Keyword', fontsize=18)\n",
    "    #plt.savefig(os.path.join(figuresFolder, 'Datasets Keyword.jpg'), bbox_inches=\"tight\")\n",
    "    plt.savefig(os.path.join(figuresFolder, 'Datasets Keyword.pdf'), bbox_inches=\"tight\")\n",
    "    plt.show(block=False)\n",
    "    plt.pause(1)\n",
    "    plt.close()\n",
    "    \n",
    "    \n",
    "    saveReportFileJson(\"Pipelines and Datasets Statistics.json\", statistic)\n",
    "    print(\"getPipelinesAndDatasetsStatisctics_v4 Done\")\n",
    "    \n",
    "#fetchPipelinesInfo_v4()            \n",
    "#fetchDatasetsInfo_v4()            \n",
    "#getPipelinesAndDatasetsStatisctics_v4()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "personalized-campaign",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fillRecommenderDataFrameStatistics_v4():\n",
    "    global recommender_DF, RECOMMENDER_DF_STATISTICS\n",
    "    loadRecommenderDataFrame()\n",
    "\n",
    "    for key in RECOMMENDER_DF_STATISTICS.keys():\n",
    "        RECOMMENDER_DF_STATISTICS[key] = 0\n",
    "    #print(json.dumps(RECOMMENDER_DF_STATISTICS, indent=4))\n",
    "\n",
    "    #print(\"Dataframe : \", recommender_DF.shape)\n",
    "    for rowName in recommender_DF.index:\n",
    "        for columnName in recommender_DF.columns:\n",
    "            RECOMMENDER_DF_STATISTICS[\"Num of all items\"] += 1\n",
    "            \n",
    "            if recommender_DF.loc[rowName, columnName][\"EXPERT_COMMENTS\"]:\n",
    "                RECOMMENDER_DF_STATISTICS[\"Num of Assessed By Experts\"] += 1\n",
    "                \n",
    "                if recommender_DF.loc[rowName, columnName][\"EXPERT_COMMENTS\"].count(\"+1\") > 0:\n",
    "                    RECOMMENDER_DF_STATISTICS[\"Num of Success Predicted By Experts\"] += 1\n",
    "            else:\n",
    "                    RECOMMENDER_DF_STATISTICS[\"Num of Not Assessed By Experts\"] += 1\n",
    "                \n",
    "            if recommender_DF.loc[rowName, columnName][\"EXECUTION_RESULT_DETAILS\"]:\n",
    "                RECOMMENDER_DF_STATISTICS[\"Num of Tests\"] += 1\n",
    "            \n",
    "            if recommender_DF.loc[rowName, columnName][\"EXECUTION_RESULT_DETAILS\"] == 'failed':\n",
    "                RECOMMENDER_DF_STATISTICS[\"Num of Tests is failed\"] += 1\n",
    "            elif recommender_DF.loc[rowName, columnName][\"EXECUTION_RESULT_DETAILS\"] == 'wrong data':\n",
    "                RECOMMENDER_DF_STATISTICS[\"Num of Tests is wrong data or wrong format\"] += 1\n",
    "            elif recommender_DF.loc[rowName, columnName][\"EXECUTION_RESULT_DETAILS\"] == 'wrong format':\n",
    "                RECOMMENDER_DF_STATISTICS[\"Num of Tests is wrong data or wrong format\"] += 1\n",
    "            elif recommender_DF.loc[rowName, columnName][\"EXECUTION_RESULT_DETAILS\"] == 'not available':\n",
    "                RECOMMENDER_DF_STATISTICS[\"Num of Tests is not available\"] += 1\n",
    "            elif recommender_DF.loc[rowName, columnName][\"EXECUTION_RESULT_DETAILS\"] == 'pipeline issue':\n",
    "                RECOMMENDER_DF_STATISTICS[\"Num of Tests is pipeline issue\"] += 1\n",
    "            elif recommender_DF.loc[rowName, columnName][\"EXECUTION_RESULT_DETAILS\"] == 'success':\n",
    "                RECOMMENDER_DF_STATISTICS[\"Num of Tests is success\"] += 1\n",
    "            else:\n",
    "                RECOMMENDER_DF_STATISTICS[\"Num of Tests is not tested\"] += 1\n",
    "    \n",
    "            if recommender_DF.loc[rowName, columnName][\"EXPERT_COMMENTS\"]:\n",
    "                if recommender_DF.loc[rowName, columnName][\"EXPERT_COMMENTS\"].count(\"+1\") == len(recommender_DF.loc[rowName, columnName][\"EXPERT_COMMENTS\"]):\n",
    "                    RECOMMENDER_DF_STATISTICS[\"Num of 100% Assessed Success\"] += 1\n",
    "            \n",
    "            if recommender_DF.loc[rowName, columnName][\"EXPERT_COMMENTS\"]:\n",
    "                RECOMMENDER_DF_STATISTICS[\"Num of Cells With Comment\"] += 1\n",
    "                RECOMMENDER_DF_STATISTICS[\"Num of Total Comments\"] += len(recommender_DF.loc[rowName, columnName][\"EXPERT_COMMENTS\"])\n",
    "            if RECOMMENDER_DF_STATISTICS[\"Num of Cells With Comment\"] > 0:\n",
    "                RECOMMENDER_DF_STATISTICS[\"Average Expert Comments per Cell\"] = RECOMMENDER_DF_STATISTICS[\"Num of Total Comments\"] / RECOMMENDER_DF_STATISTICS[\"Num of Cells With Comment\"]\n",
    "                \n",
    "                \n",
    "    saveReportFileJson(\"Recommender Statistics.json\", RECOMMENDER_DF_STATISTICS)\n",
    "    #print(json.dumps(RECOMMENDER_DF_STATISTICS, indent=4))\n",
    "    print(\"fillRecommenderDataFrameStatistics_v4 Done\")\n",
    "                \n",
    "#fillRecommenderDataFrameStatistics_v4()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "endangered-university",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Show \n",
    "def pipelineExecution_Level5(recommenderDataFrameItam):\n",
    "    \n",
    "    MATRIX_FACTORIZATION_CODES = {\n",
    "        'failed': 1/5, \n",
    "        'wrong data': 2/5, \n",
    "        'wrong format': 2/5, \n",
    "        'not available': 3/5, \n",
    "        'pipeline issue': 4/5,\n",
    "        'success': 5/5,\n",
    "        numpy.nan: 5/5,\n",
    "        ' ': 0\n",
    "        }\n",
    "    \n",
    "    #if recommenderDataFrameItam[\"EXECUTION_RESULT_DETAILS\"] in [numpy.nan, 'success']:\n",
    "    #    return 1\n",
    "    if recommenderDataFrameItam[\"EXECUTION_RESULT_DETAILS\"] in MATRIX_FACTORIZATION_CODES.keys():\n",
    "        return MATRIX_FACTORIZATION_CODES[recommenderDataFrameItam[\"EXECUTION_RESULT_DETAILS\"]]\n",
    "    else:\n",
    "        return 0\n",
    "    return 0\n",
    "\n",
    "def pipelineExecutionFigure5_v4():\n",
    "    categorized_color_matplotlib = ['#ddd7d8', '#f9383e', '#f9595e', '#fe777b', '#fc9497', \"green\"] \n",
    "\n",
    "    sampleFigure = showFigure_Mtplotlib_v4(pipelineExecution_Level5,\n",
    "                useIndex = True,\n",
    "                colorScale = categorized_color_matplotlib, \n",
    "                result_DF_FN = os.path.join(outputsFolder, \"Figure pipelineExecution_Level5.csv\"),\n",
    "                colorbarTextPoints=[1/12, 3/12, 5/12, 7/12, 9/12, 11/12],\n",
    "                colorbarTexts=[\"Not Tried\",\"Failed\",\"Wrong Data\", \"Not Available\",\"Pipeline Issue\", \"Success\"],\n",
    "                colorbarTextRotation = 0, \n",
    "                figureTitle = \"Executed Pipelines\",\n",
    "                xlabel = \"Datasets\",\n",
    "                ylabel = \"Pipelines\")  \n",
    "\n",
    "    #sampleFigure.savefig(os.path.join(figuresFolder,\"Pipeline Execution Level 5.png\"))\n",
    "    sampleFigure.savefig(os.path.join(figuresFolder,\"Pipeline Execution Level 5.pdf\"))\n",
    "\n",
    "    #sampleFigure.show()\n",
    "\n",
    "#pipelineExecutionFigure5_v4()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "certain-character",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Show \n",
    "def pipelineExecution_Level2(recommenderDataFrameItam):\n",
    "    \n",
    "    if recommenderDataFrameItam[\"EXECUTION_RESULT_DETAILS\"] in [numpy.nan, 'success']:\n",
    "        return 1\n",
    "    elif recommenderDataFrameItam[\"EXECUTION_RESULT_DETAILS\"] in ['failed','wrong data','wrong format','not available', 'pipeline issue']:\n",
    "        return 0.5\n",
    "    else:\n",
    "        return 0\n",
    "    return 0\n",
    "\n",
    "def pipelineExecutionFigure2_v4():\n",
    "    categorized_color_matplotlib = ['#ddd7d8', '#f9383e', 'green'] \n",
    "    \n",
    "    sampleFigure = showFigure_Mtplotlib_v4(pipelineExecution_Level2,\n",
    "                useIndex = True,\n",
    "                colorScale = categorized_color_matplotlib, \n",
    "                result_DF_FN = os.path.join(outputsFolder, \"Figure pipelineExecution_Level2.csv\"),\n",
    "                colorbarTextPoints=[1/6, 3/6, 5/6],\n",
    "                colorbarTexts=[\"No Data\",\"Failed\",\"Success\"],\n",
    "                colorbarTextRotation = 0, \n",
    "                figureTitle = \"Executed Pipelines\",\n",
    "                xlabel = \"Datasets\",\n",
    "                ylabel = \"Pipelines\")  \n",
    "\n",
    "    #sampleFigure.savefig(os.path.join(figuresFolder,\"Pipeline Execution Level 2.png\"))\n",
    "    sampleFigure.savefig(os.path.join(figuresFolder,\"Pipeline Execution Level 2.pdf\"))\n",
    "\n",
    "    #sampleFigure.show()\n",
    "#pipelineExecutionFigure2_v4()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "palestinian-consciousness",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Show \n",
    "'''\n",
    "pipelineDict = getPipelinesIndexDict()    \n",
    "datasetDict = getDatasetsIndexDict()    \n",
    "for rowName in recommender_DF.index:\n",
    "    for columnName in recommender_DF.columns:\n",
    "         if (pipelineDict[rowName] == \"P4\") and ( datasetDict[columnName] == \"D0\"):\n",
    "                print(recommender_DF.loc[rowName, columnName])\n",
    "        #if ['FRACTION_RATE_SUCCESS_OVER_COMMENTED'] == 0: \n",
    "'''\n",
    "\n",
    "percentageValues = []\n",
    "def calculateSuccessPercentage(recommenderDataFrameItam):\n",
    "    #global percentageValues, pipelineDict, datasetDict\n",
    "    percentageValues = {-1: 0, 0.0: 1/7, 0.25: 2/7, 0.33: 3/7, 0.5: 4/7, 0.67: 5/7, 0.75: 6/7, 1.0: 7/7}\n",
    "    \n",
    "    \n",
    "    result = 0\n",
    "    if not recommenderDataFrameItam[\"EXPERT_COMMENTS\"]: #If no comments\n",
    "        result = -1\n",
    "    else:\n",
    "        result = recommenderDataFrameItam[\"EXPERT_COMMENTS\"].count('+1')/len(recommenderDataFrameItam[\"EXPERT_COMMENTS\"])\n",
    "        #if result == 0:\n",
    "        #    print(\"Result :\\n\", recommenderDataFrameItam, \"\\n\\n\")\n",
    "            \n",
    "    #if round(result, 2) not in percentageValues:\n",
    "    #    percentageValues += [round(result, 2)]\n",
    "    result = percentageValues[round(result, 2)]\n",
    "    return result\n",
    "\n",
    "def generateColormap(startRGB, stopRGB, numOfColors):\n",
    "    startR, startG, startB = list(ImageColor.getcolor(startRGB, \"RGB\"))\n",
    "    startR, startG, startB = list(ImageColor.getrgb(startRGB))\n",
    "    stopR, stopG, stopB = list(ImageColor.getrgb(stopRGB))\n",
    "    colorArray = []\n",
    "    for i in range(numOfColors):\n",
    "        colorArray += ['#%02x%02x%02x' % (int(startR + (stopR - startR) * i / numOfColors), int(startG + (stopG - startG) * i / numOfColors), int(startB + (stopB - startB) * i / numOfColors))]\n",
    "    return colorArray\n",
    " \n",
    "           \n",
    "def successPercentageFigure_v4():    \n",
    "    global percentageValues\n",
    "    \n",
    "    categorized_color_matplotlib = ['#bfbfbf', '#ffffff', '#006600'] \n",
    "    \n",
    "    percentageValues = [-1, 0.0, 0.25, 0.33, 0.5, 0.67, 0.75, 1.0]\n",
    "    categorized_color_matplotlib = ['#bfbfbf'] * 7 +  generateColormap('#ffffff', '#006600', 7) \n",
    "    categorized_color_matplotlib = ['#bfbfbf'] +  generateColormap('#ffffff', '#006600', 7) \n",
    "    #print(\"categorized_color_matplotlib : \", categorized_color_matplotlib)\n",
    "    \n",
    "    sampleFigure = showFigure_Mtplotlib_v4_For_Percentage(calculateSuccessPercentage,\n",
    "                useIndex = True,\n",
    "                colorScale = categorized_color_matplotlib, \n",
    "                result_DF_FN = os.path.join(outputsFolder, \"Commented Success Percentage.csv\"),\n",
    "                colorbarTextPoints=[1/16, 3/16, 15/16],\n",
    "                colorbarTexts=[\"No Data\", \"0%\",\"100%\"],\n",
    "                colorbarTextRotation = 0,\n",
    "                xlabel = \"Datasets\",\n",
    "                ylabel = \"Pipelines\",\n",
    "                figureTitle = \"Experts Predictions\")  \n",
    "    #print(\"Percentage Values : \", sorted(percentageValues))\n",
    "    \n",
    "    percentageValues = [-1, 0.0, 0.25, 0.33, 0.5, 0.67, 0.75, 1.0]\n",
    "    #print(type(sampleFigure))\n",
    "    \n",
    "    '''\n",
    "    import matplotlib.pyplot as plt\n",
    "    import matplotlib as mpl\n",
    "\n",
    "    img, axe = plt.subplots(figsize=(6, 1))\n",
    "    cmap = mpl.colors.ListedColormap(['red', 'green', 'blue', 'cyan'])\n",
    "    bounds = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "    #norm = mpl.colors.BoundaryNorm(bounds, cmap.N)\n",
    "    cb2 = mpl.colorbar.ColorbarBase(axe, cmap=cmap,\n",
    "                                    #norm=norm,\n",
    "                                    boundaries=bounds,\n",
    "                                    #extend='both',\n",
    "                                    ticks=bounds,\n",
    "                                    spacing='proportional',\n",
    "                                    orientation='horizontal')\n",
    "    cb2.set_label('Discrete intervals, some other units')\n",
    "\n",
    "    sampleFigure.colorbar(ax=ax)\n",
    "    sampleFigure.axes[1] = img\n",
    "    cbar = img\n",
    "    #sampleFigure.colorbar(axe)\n",
    "    sampleFigure.draw()\n",
    "     '''\n",
    "    \n",
    "    #sampleFigure.tight_layout()\n",
    "    #sampleFigure.savefig(os.path.join(figuresFolder,\"Commented Success Percentage.png\"))\n",
    "    sampleFigure.savefig(os.path.join(figuresFolder,\"Commented Success Percentage.pdf\"))#, bbox_inches='tight'))\n",
    "    #sampleFigure.show()\n",
    "    \n",
    "#successPercentageFigure_v4()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "progressive-scoop",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.ticker import (MultipleLocator, AutoMinorLocator)\n",
    "\n",
    "\n",
    "def exprtConfidenceSwarmPlot_v4():\n",
    "    testConfidence = pd.read_csv(confidenceEvaluationReport_DF_FN)\n",
    "    #fig, ax = plt.subplots()\n",
    "    \n",
    "    snsPlot = sns.boxplot(x=\"Status\", y=\"Average expert confidence\", data=testConfidence, boxprops={'facecolor':'None'})#, ax=ax)\n",
    "    snsPlot = sns.swarmplot(x=\"Status\", y=\"Average expert confidence\", data=testConfidence,palette=sns.color_palette(['red', 'green']), zorder=.5)#, ax=ax)\n",
    "\n",
    "    snsPlot.set(xlabel=None)\n",
    "    snsPlot.set_yticklabels(['','Good','','','','','Advanced'])\n",
    "    snsPlot.set_xticklabels(snsPlot.get_xmajorticklabels(),fontsize=15)\n",
    "    snsPlot.set_yticklabels(snsPlot.get_ymajorticklabels(),fontsize=15)\n",
    "    snsPlot.set_ylabel(\"Average expert confidence\",fontsize=15)\n",
    "    \n",
    "    frame1 = plt.gca()\n",
    "    '''\n",
    "    for xlabel_i in frame1.axes.get_xticklabels():\n",
    "        xlabel_i.set_visible(False)\n",
    "        xlabel_i.set_fontsize(0.0)\n",
    "    for xlabel_i in frame1.axes.get_yticklabels():\n",
    "        xlabel_i.set_fontsize(0.0)\n",
    "        xlabel_i.set_visible(False)\n",
    "    for tick in frame1.axes.get_xticklines():\n",
    "        tick.set_visible(False)'''\n",
    "    for index, tick in enumerate(frame1.axes.get_yticklines()):\n",
    "        if (index > 2) and (index < len(frame1.axes.get_yticklines()) - 4):\n",
    "            tick.set_visible(False)\n",
    "        else:\n",
    "            tick.set_visible(True)\n",
    "            \n",
    "    #snsPlot.set_yticklabels(['Good','Advanced'])\n",
    "    #snsPlot.yaxis.set_ticks_position('both')\n",
    "    #snsPlot.yaxis.set_visible(False)\n",
    "    \n",
    "    plt.savefig(os.path.join(figuresFolder,'Confidence Swarm Experts.pdf'),bbox_inches = 'tight')\n",
    "    #plt.savefig(os.path.join(figuresFolder,'Confidence Swarm Experts.png'),bbox_inches = 'tight')\n",
    "    plt.show(block=False)\n",
    "    plt.pause(1)\n",
    "    plt.close()\n",
    "    \n",
    "    \n",
    "    successDF = testConfidence.loc[testConfidence['Status'] == 'success']\n",
    "    failDF = testConfidence.loc[testConfidence['Status'] == 'failed']\n",
    "\n",
    "    successConfidences = successDF['Average expert confidence'].tolist()\n",
    "    failConfidences = failDF['Average expert confidence'].tolist()\n",
    "\n",
    "    #print(mean(successConfidences))\n",
    "    #print(mean(failConfidences))\n",
    "\n",
    "    stat, p = ttest_ind(failConfidences,successConfidences)\n",
    "    #print('stat: ', stat)\n",
    "    #print('p: ', p) #A p-value is the probability that the results from your sample data occurred by chance, In most cases, a p-value of 0.05 (5%) is accepted to mean the data is valid.\n",
    "    \n",
    "    print(\"exprtConfidenceSwarmPlot_v4 Done\")\n",
    "    \n",
    "def exprtConfidenceStackedBar_v4_():\n",
    "    testConfidence = pd.read_csv(confidenceEvaluationReport_DF_FN)\n",
    "    #fig, ax = plt.subplots()\n",
    "    figure, ax = plt.subplots(1, 1, figsize=(8, 8), dpi=60)\n",
    "    \n",
    "    confidenceValues = sorted(list(dict.fromkeys(testConfidence['Average expert confidence'].tolist())))\n",
    "    confidenceValues = [round(num, 2) for num in confidenceValues]\n",
    "    #print(confidenceValues)\n",
    "    \n",
    "    confidenceKeys = []\n",
    "    for item in confidenceValues:\n",
    "        confidenceKeys += ['success ' + str(round(item, 2))]\n",
    "        confidenceKeys += ['failed ' + str(round(item, 2))]\n",
    "    confidenceStatistic = dict.fromkeys(confidenceKeys, 0)\n",
    "    #print(confidenceStatistic)\n",
    "    \n",
    "    for rowName in testConfidence.index:\n",
    "        key = testConfidence.loc[rowName]['Status'].lower() + ' ' + str(round(testConfidence.loc[rowName]['Average expert confidence'], 2))\n",
    "        confidenceStatistic[key] += 1\n",
    "            \n",
    "    #print(confidenceStatistic)\n",
    "    columnList = []\n",
    "    for value in confidenceValues:\n",
    "        columnList += [[confidenceStatistic['success ' + str(value)], confidenceStatistic['failed ' + str(value)]]]\n",
    "        \n",
    "    #print(columnList)\n",
    "    colors = [\"#355ADE\", \"#EB54DE\", \"#FFEDA7\", \"#5BBA56\", \"#B16B69\"]\n",
    "    colors = [\"#88CCEE\", \"#CC6677\", \"#DDCC77\", \"#117733\", \"#332288\", \"#AA4499\", \"#44AA99\", \"#999933\"]\n",
    "    \n",
    "    \n",
    "    bottom = np.array([0, 0])\n",
    "    #print(bottom)\n",
    "    for index, item in enumerate(columnList):\n",
    "        if index == 0:\n",
    "            plt.bar(['success', 'failed'], columnList[index], color=colors[index])\n",
    "        else:\n",
    "            bottom += np.array(columnList[index - 1])\n",
    "            #print(bottom)\n",
    "            plt.bar(['success', 'failed'], columnList[index], bottom = bottom, color=colors[index])\n",
    "    \n",
    "    plt.legend([str(confidenceValue) for confidenceValue in confidenceValues], title=\"Confidence Values\", loc=2, fontsize = 6)#, prop={'size': 6})\n",
    "    plt.title(\"Expert Confidence for Executed Pipelines\", fontsize=13)\n",
    "\n",
    "    ax.set_yticks(np.round([0, 160]), minor=False)\n",
    "    ax.set_yticklabels(['Good', 'Advanced'], fontsize=13)\n",
    "    \n",
    "    ax.set_xticklabels(['Success', 'Failed'], fontsize=13)\n",
    "    \n",
    "    plt.xlabel('Execution Status', fontsize=13, labelpad=2)\n",
    "    plt.ylabel('Experts Knowledge Level', fontsize=13, labelpad=2)\n",
    "        \n",
    "    plt.savefig(os.path.join(figuresFolder,'Confidence Stacked Bar Experts.pdf'),bbox_inches = 'tight')\n",
    "    plt.show(block=False)\n",
    "    plt.pause(1)\n",
    "    plt.close()\n",
    "    \n",
    "    print(\"exprtConfidenceStackedBar_v4 Done\")\n",
    "\n",
    "def exprtConfidenceStackedBar_v4():\n",
    "    testConfidence = pd.read_csv(confidenceEvaluationReport_DF_FN)\n",
    "    #fig, ax = plt.subplots()\n",
    "    figure, ax = plt.subplots(1, 1, figsize=(8, 8), dpi=60)\n",
    "    \n",
    "    confidenceValues = sorted(list(dict.fromkeys(testConfidence['Average expert confidence'].tolist())))\n",
    "    confidenceValues = [round(num, 2) for num in confidenceValues]\n",
    "    #print(confidenceValues)\n",
    "    \n",
    "    confidenceKeys = []\n",
    "    for item in confidenceValues:\n",
    "        confidenceKeys += ['success ' + str(round(item, 2))]\n",
    "        confidenceKeys += ['failed ' + str(round(item, 2))]\n",
    "    confidenceStatistic = dict.fromkeys(confidenceKeys, 0)\n",
    "    #print(confidenceStatistic)\n",
    "    \n",
    "    for rowName in testConfidence.index:\n",
    "        key = testConfidence.loc[rowName]['Status'].lower() + ' ' + str(round(testConfidence.loc[rowName]['Average expert confidence'], 2))\n",
    "        confidenceStatistic[key] += 1\n",
    "            \n",
    "    #print(confidenceStatistic)\n",
    "    columnList = [[confidenceStatistic['success ' + str(value)] for value in confidenceValues], [confidenceStatistic['failed ' + str(value)] for value in confidenceValues]]\n",
    "        \n",
    "    #print(columnList)\n",
    "    rowLabel = []\n",
    "    for value in confidenceValues:\n",
    "        rowLabel += [str(value)]\n",
    "    #print(rowLabel)\n",
    "    \n",
    "    plt.bar(rowLabel, columnList[0], color='green')\n",
    "    plt.bar(rowLabel, columnList[1], bottom = np.array(columnList[0]), color='red')\n",
    "    \n",
    "    plt.legend(['Success', 'Failed'], title=\"Confidence Values\", loc=2, fontsize = 6)#, prop={'size': 6})\n",
    "    plt.title(\"Expert Confidence for Executed Pipelines\", fontsize=13)\n",
    "\n",
    "    ax.set_xticks(np.round([0, 5]), minor=False)\n",
    "    ax.set_xticklabels(['Good', 'Advanced'], fontsize=13)\n",
    "    '''\n",
    "    ax.set_xticklabels(['Success', 'Failed'], fontsize=13)\n",
    "    '''\n",
    "    \n",
    "    plt.ylabel('Number of Executions', fontsize=13, labelpad=2)\n",
    "    plt.xlabel('Experts Knowledge Level', fontsize=13, labelpad=2)\n",
    "        \n",
    "    plt.savefig(os.path.join(figuresFolder,'Confidence Stacked Bar Experts.pdf'),bbox_inches = 'tight')\n",
    "    plt.show(block=False)\n",
    "    plt.pause(1)\n",
    "    plt.close()\n",
    "    \n",
    "    print(\"exprtConfidenceStackedBar_v4 Done\")\n",
    "    \n",
    "#exprtConfidenceStackedBar_v4()\n",
    "def expertConfidenceGroupedBar_v4():\n",
    "    \n",
    "    testConfidence = pd.read_csv(confidenceEvaluationReport_DF_FN)\n",
    "    #fig, ax = plt.subplots()\n",
    "    figure, ax = plt.subplots(1, 1, figsize=(8, 8), dpi=60)\n",
    "    \n",
    "    confidenceValues = sorted(list(dict.fromkeys(testConfidence['Average expert confidence'].tolist())))\n",
    "    confidenceValues = [round(num, 2) for num in confidenceValues]\n",
    "    #print(confidenceValues)\n",
    "    \n",
    "    confidenceKeys = []\n",
    "    for item in confidenceValues:\n",
    "        confidenceKeys += ['success ' + str(round(item, 2))]\n",
    "        confidenceKeys += ['failed ' + str(round(item, 2))]\n",
    "    confidenceStatistic = dict.fromkeys(confidenceKeys, 0)\n",
    "    #print(confidenceStatistic)\n",
    "    \n",
    "    for rowName in testConfidence.index:\n",
    "        key = testConfidence.loc[rowName]['Status'].lower() + ' ' + str(round(testConfidence.loc[rowName]['Average expert confidence'], 2))\n",
    "        confidenceStatistic[key] += 1\n",
    "            \n",
    "    #print(confidenceStatistic)\n",
    "    columnList = [[confidenceStatistic['success ' + str(value)] for value in confidenceValues], [confidenceStatistic['failed ' + str(value)] for value in confidenceValues]]\n",
    "        \n",
    "    #print(columnList)\n",
    "    rowLabel = []\n",
    "    for value in confidenceValues:\n",
    "        rowLabel += [value]\n",
    "    #print(rowLabel)\n",
    "    \n",
    "    width = 0.05  # the width of the bars\n",
    "\n",
    "    #print(columnList[0])\n",
    "    #print(columnList[1])\n",
    "    \n",
    "    #print([item  - width/2 for item in rowLabel])\n",
    "    rects1 = ax.bar([item  - width/2 for item in rowLabel], columnList[0], width, label='Success', color='green')\n",
    "    rects2 = ax.bar([item  + width/2 for item in rowLabel], columnList[1], width, label='Failed', color='red')\n",
    "\n",
    "    plt.legend(['Success', 'Failed'], loc=2, fontsize = 15)#, prop={'size': 6})\n",
    "    #plt.title(\"Expert Confidence for Executed Pipelines\", fontsize=15)\n",
    "\n",
    "    # Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "    ax.set_ylabel('Scores')\n",
    "    #ax.set_title('Scores by group and gender')\n",
    "    ax.set_xticks(np.round([2.9, 4.1]), minor=False)\n",
    "    ax.set_xticklabels(['Good', 'Advanced'], fontsize=15)\n",
    "    ax.legend(fontsize=15)\n",
    "    \n",
    "    plt.ylabel('Number of Executions', fontsize=15, labelpad=2)\n",
    "    plt.xlabel('Experts Knowledge Level', fontsize=15, labelpad=2)\n",
    "     \n",
    "    #ax.bar_label(rects1, padding=3)\n",
    "    #ax.bar_label(rects2, padding=3)\n",
    "\n",
    "    figure.tight_layout()\n",
    "    plt.savefig(os.path.join(figuresFolder,'Confidence Grouped Bar Experts.pdf'),bbox_inches = 'tight')\n",
    "\n",
    "    plt.show(block=False)\n",
    "    plt.pause(1)\n",
    "    plt.close()\n",
    "    \n",
    "    print(\"expertConfidenceGroupedBar_v4 Done\")\n",
    "    \n",
    "#expertConfidenceGroupedBar_v4()\n",
    "\n",
    "def exprtFractionSwarmPlot_v4():\n",
    "    testConfidence = pd.read_csv(fractionRateReport_DF_FN)\n",
    "    #fig, ax = plt.subplots()\n",
    "    \n",
    "    snsPlot = sns.boxplot(testConfidence[\"Status\"], y=\"Fraction of experts\", data=testConfidence, boxprops={'facecolor':'None'})#, ax=ax)\n",
    "    snsPlot = sns.swarmplot(testConfidence[\"Status\"], y=\"Fraction of experts\", data=testConfidence,palette=sns.color_palette(['red', 'green']), zorder=.05)#, ax=ax)\n",
    "\n",
    "    snsPlot.set(xlabel=None)\n",
    "    #snsPlot.tick_params(labelsize=15)\n",
    "    snsPlot.set_xticklabels(snsPlot.get_xmajorticklabels(),fontsize=15)\n",
    "\n",
    "    snsPlot.set_ylabel(\"Fraction of experts\",fontsize=15)\n",
    "    \n",
    "    plt.savefig(os.path.join(figuresFolder,'Fraction of experts.pdf'))\n",
    "    #plt.savefig(os.path.join(figuresFolder,'Fraction of experts.png'))\n",
    "    plt.show(block=False)\n",
    "    plt.pause(1)\n",
    "    plt.close()\n",
    "    \n",
    "    successDF = testConfidence.loc[testConfidence['Status'] == 'success']\n",
    "    failDF = testConfidence.loc[testConfidence['Status'] == 'failed']\n",
    "\n",
    "    successConfidences = successDF['Fraction of experts'].tolist()\n",
    "    failConfidences = failDF['Fraction of experts'].tolist()\n",
    "\n",
    "    #print(mean(successConfidences))\n",
    "    #print(mean(failConfidences))\n",
    "\n",
    "    stat, p = ttest_ind(failConfidences,successConfidences)\n",
    "    #print('stat: ', stat)\n",
    "    #print('p: ', p) #A p-value is the probability that the results from your sample data occurred by chance, In most cases, a p-value of 0.05 (5%) is accepted to mean the data is valid.\n",
    "    \n",
    "    print(\"confidenceSwarmPlot_v4 Done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "successful-modem",
   "metadata": {},
   "outputs": [],
   "source": [
    "def TPR_FPR_experts(predictions):\n",
    "    Posetive_class2 = predictions[predictions['Status'] == 'success']\n",
    "    Negative_class1 = predictions[predictions['Status'] == 'failed']\n",
    "    TP_df = predictions[(predictions['Status'] == 'success') & (predictions['pred_class'] == 'success')]\n",
    "    FP_df = predictions[(predictions['Status'] == 'failed') & (predictions['pred_class'] == 'success')]\n",
    "    TN_df = predictions[(predictions['Status'] == 'failed') & (predictions['pred_class'] == 'failed')]\n",
    "    FN_df = predictions[(predictions['Status'] == 'success') & (predictions['pred_class'] == 'failed')]\n",
    "\n",
    "    TPR = TP_df.shape[0]/Posetive_class2.shape[0]\n",
    "    FPR = FP_df.shape[0]/Negative_class1.shape[0]\n",
    "    return TPR,FPR\n",
    "\n",
    "\n",
    "def allThresholds_experts(predictions):   # gets df of all predictions and the fraction of experts\n",
    "    thre_TPR_FPR = pd.DataFrame(columns = ['threshold','avg_TPR','avg_FPR'])\n",
    "\n",
    "    all_avg_TPR = []\n",
    "    all_avg_FPR = []\n",
    "    TPR_list = []\n",
    "    FPR_list = []\n",
    "    i = 0\n",
    "    for threshold in np.arange(0,1.1,0.1):\n",
    "        \n",
    "        \n",
    "        predictions.loc[predictions['Fraction of experts'] <= threshold, 'pred_class'] = 'failed'\n",
    "        predictions.loc[predictions['Fraction of experts'] > threshold, 'pred_class'] = 'success'\n",
    "        TPR,FPR = TPR_FPR_experts(predictions)\n",
    "        TPR_list.append(TPR)\n",
    "        FPR_list.append(FPR)\n",
    "    return TPR_list,FPR_list\n",
    "\n",
    "def TPR_FPR_recommender(predictions):\n",
    "    \n",
    "    type(predictions)\n",
    "    Posetive_class2 = predictions.loc[predictions['status'] == 2]\n",
    "    Negative_class1 = predictions.loc[predictions['status'] == 1]\n",
    "    TP_df = predictions.loc[(predictions['status'] == 2) & (predictions['pred_class'] == 2)]\n",
    "    FP_df = predictions.loc[(predictions['status'] == 1) & (predictions['pred_class'] == 2)]\n",
    "    TN_df = predictions.loc[(predictions['status'] == 1) & (predictions['pred_class'] == 1)]\n",
    "    FN_df = predictions.loc[(predictions['status'] == 2) & (predictions['pred_class'] == 1)]\n",
    "    \n",
    "\n",
    "    TPR = TP_df.shape[0]/Posetive_class2.shape[0]\n",
    "    FPR = FP_df.shape[0]/Negative_class1.shape[0]\n",
    "    return TPR,FPR\n",
    "\n",
    "\n",
    "def allThresholds_recommender(allFoldsPredictions):#,minTh,maxTh):\n",
    "    thre_TPR_FPR = pd.DataFrame(columns = ['threshold','avg_TPR','avg_FPR'])\n",
    "\n",
    "    all_avg_TPR = []\n",
    "    all_avg_FPR = []\n",
    "    i = 0\n",
    "    TPR_list = []\n",
    "    FPR_list = []\n",
    "    for threshold in np.arange(-0.1,2.4,0.1):\n",
    "        allFoldsPredictions['pred_class'] = np.where(allFoldsPredictions['prediction'] > threshold, 2, 1)\n",
    "        TPR,FPR = TPR_FPR_recommender(allFoldsPredictions)\n",
    "        TPR_list.append(TPR)\n",
    "        FPR_list.append(FPR)\n",
    "    avg_TPR = sum(TPR_list)/len(TPR_list)\n",
    "    all_avg_TPR.append(avg_TPR)\n",
    "    avg_FPR = sum(FPR_list)/len(FPR_list)\n",
    "    all_avg_FPR.append(avg_FPR)\n",
    "    print(\"threshold\", threshold,\"avg_TPR\",avg_TPR,\"avg_FPR: \",avg_FPR)\n",
    "    #thre_TPR_FPR = thre_TPR_FPR.append({'threshold':threshold,'avg_TPR': avg_TPR ,'avg_FPR': avg_FPR}, ignore_index = True)\n",
    "\n",
    "    return TPR_list,FPR_list\n",
    "\n",
    "def rocCurveEvaluation_v4():\n",
    "    df = pd.read_csv(fractionRateReport_DF_FN)\n",
    "    \n",
    "    #fig, ax = plt.subplots()\n",
    "    \n",
    "    \n",
    "    tpr,fpr = allThresholds_experts(df)\n",
    "    AUC_experts = 0\n",
    "    for i in range(len(tpr)-1):\n",
    "        AUC_experts = AUC_experts + (((tpr[i]+tpr[i+1])/2) * (fpr[i]-fpr[i+1]))\n",
    "    print(AUC_experts) \n",
    "    \n",
    "    #https://github.com/matplotlib/matplotlib/issues/9460\n",
    "    #colorlind codes : '006BA4', 'FF800E', 'ABABAB', '595959', '5F9ED1', 'C85200', '898989', 'A2C8EC', 'FFBC79', 'CFCFCF'\n",
    "    #colors = [\"#88CCEE\", \"#CC6677\", \"#DDCC77\", \"#117733\", \"#332288\", \"#AA4499\", \"#44AA99\", \"#999933\"]\n",
    "    \n",
    "    plt.plot(fpr,tpr, color='#006BA4')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.xlim([0, 1])\n",
    "    plt.ylim([0, 1])\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "\n",
    "    #plt.savefig('thre_1-2.png')\n",
    "\n",
    "    #plt.show(block=False)\n",
    "    \n",
    "    allPredictions = pd.read_csv(alsModel_FN)\n",
    "    all_avg_TPR,all_avg_FPR = allThresholds_recommender(allPredictions)\n",
    "    AUC = 0\n",
    "    for i in range(len(all_avg_TPR)-1):\n",
    "        AUC = AUC + (((all_avg_TPR[i]+all_avg_TPR[i+1])/2) * (all_avg_FPR[i]-all_avg_FPR[i+1]))\n",
    "    print(AUC) \n",
    "\n",
    "    plt.plot(all_avg_FPR,all_avg_TPR, label='Recommender System', color='#26BDE6')\n",
    "    plt.plot(fpr,tpr, label='Experts', color='#D048E8')\n",
    "    plt.plot([0, 1], [0, 1], '--', label='Chance', color='#DB5821')\n",
    "    #plt.plot([], [], '--', label='Chance', color='#FF800E')\n",
    "\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.plot([0, 1], [0, 1], '--', label='Chance', color='#DB5821')\n",
    "    plt.xlim([0, 1])\n",
    "    plt.ylim([0, 1])\n",
    "    plt.ylabel('True Positive Rate',fontsize=14)\n",
    "    plt.xlabel('False Positive Rate',fontsize=14)\n",
    "    plt.plot(0.23, 0.81, marker='o', markersize=10,  color='#4D4D4D')\n",
    "    \n",
    "    plt.savefig(os.path.join(figuresFolder, \"ROC Curve.pdf\"))\n",
    "    plt.show(block=False)\n",
    "    plt.pause(1)\n",
    "    plt.close()\n",
    "\n",
    "    print(\"rocCurveEvaluation_v4 Done\")\n",
    "\n",
    "\n",
    "#rocCurveEvaluation_v4()   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "formed-advocacy",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "def scientificRecommender_v4():\n",
    "    concatenateExpertComments_v4() \n",
    "    addExpertsConfidence_v4()\n",
    "    trimDataframe_v4()\n",
    "    extractIndexTables_v4()\n",
    "    addConfidenceRatings_v4()\n",
    "    addExecutionResult_v4()\n",
    "    generateConfidenceEvaluationReport_v4() \n",
    "    addFractionRates_v4() \n",
    "    \n",
    "    generateAlsModelInput_v4()\n",
    "    \n",
    "    #This function generates statistic reports for Pipelines and Datasets based on fetched information from the server\n",
    "    #fetchPipelinesInfo_v4()            \n",
    "    #fetchDatasetsInfo_v4()            \n",
    "    #getPipelinesAndDatasetsStatisctics_v4() \n",
    "\n",
    "    fillRecommenderDataFrameStatistics_v4()\n",
    "    #pipelineExecutionFigure5_v4()\n",
    "    pipelineExecutionFigure2_v4()\n",
    "    successPercentageFigure_v4()\n",
    "\n",
    "    #exprtConfidenceSwarmPlot_v4()\n",
    "    #exprtConfidenceStackedBar_v4()\n",
    "    expertConfidenceGroupedBar_v4()\n",
    "    #exprtFractionSwarmPlot_v4()\n",
    "\n",
    "    rocCurveEvaluation_v4() \n",
    "    \n",
    "#scientificRecommender_v4()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "pressing-maintenance",
   "metadata": {},
   "outputs": [],
   "source": [
    "def showFigure_Mtplotlib_v4_(\n",
    "    processingFunction, \n",
    "    useIndex = False,\n",
    "    colorScale=[[0, \"red\"], [0.5 , \"white\"], [1, \"green\"]], \n",
    "    figureTitle=None,\n",
    "    result_DF_FN = None,\n",
    "    colorbarTextPoints = None,\n",
    "    colorbarTexts = None,\n",
    "    colorbarTextRotation = 0):\n",
    "    \n",
    "\n",
    "    global recommender_DF\n",
    "    if recommender_DF.empty:\n",
    "        loadRecommenderDataFrame()\n",
    "    \n",
    "    if not processingFunction:\n",
    "        return\n",
    "    \n",
    "    temporary_DF = pd.DataFrame(index=recommender_DF.index.tolist(), columns=recommender_DF.columns.tolist())\n",
    "    \n",
    "    for rowName in recommender_DF.index:\n",
    "        for columnName in recommender_DF.columns:\n",
    "            #temporary_DF.loc[rowName, columnName] = 2 * processingFunction(recommender_DF.loc[rowName, columnName]) - 1  #Map (0, 1) to (-1, 1)\n",
    "            temporary_DF.loc[rowName, columnName] = processingFunction(recommender_DF.loc[rowName, columnName])\n",
    "    \n",
    "    if useIndex is True:\n",
    "        temporary_DF = indexDataFrame_v4(temporary_DF)\n",
    "    \n",
    "    if result_DF_FN:\n",
    "        if '.xlsx' in result_DF_FN:\n",
    "            temporary_DF.to_excel(result_DF_FN)\n",
    "        elif '.csv' in result_DF_FN:\n",
    "            temporary_DF.to_csv(\n",
    "                result_DF_FN, \n",
    "                sep=',', \n",
    "                encoding='utf-8')\n",
    "\n",
    "    temporary_DF = temporary_DF.astype(float)\n",
    "    \n",
    "    #figure = plt.figure(figsize=(20, 20))\n",
    "    #figure, ax = plt.subplots(figsize=(18, 18))\n",
    "    figure, ax = plt.subplots(figsize=(15, 18))\n",
    "    \n",
    "    ax = sns.heatmap(\n",
    "        temporary_DF, \n",
    "        cmap=ListedColormap(colorScale), \n",
    "        vmin=0, \n",
    "        vmax=1, \n",
    "        #linewidths=.5,\n",
    "        cbar=True,\n",
    "        cbar_kws={\n",
    "            'pad': 0.05, \n",
    "            #'orientation': 'horizontal',\n",
    "            'orientation': 'vertical',\n",
    "            \"shrink\": 0.5\n",
    "        },\n",
    "        ax = ax,\n",
    "        square=True, #Square the tiles\n",
    "        )\n",
    "    \n",
    "    \n",
    "    cmap = ax.collections[0].cmap\n",
    "    cmap.set_under('white')#Colour values less than vmin in white\n",
    "    cmap.set_over('yellow')# colour valued larger than vmax in red\n",
    "    \n",
    "    #ax.set_aspect('equal') #Square the tiles\n",
    "    \n",
    "    if figureTitle:\n",
    "        plt.title(title=figureTitle)\n",
    "        \n",
    "    plt.yticks(\n",
    "        np.arange(0.5, len(temporary_DF.index), 1), \n",
    "        temporary_DF.index, \n",
    "        rotation = 0,\n",
    "        fontsize=15)\n",
    "\n",
    "    plt.xticks(\n",
    "        np.arange(0.5, len(temporary_DF.columns), 1),\n",
    "        temporary_DF.columns, \n",
    "        rotation = 90,\n",
    "        fontsize=15)\n",
    "    \n",
    "    #Update colorbar\n",
    "    if colorbarTextPoints and colorbarTexts:\n",
    "        #Get access to colorbar\n",
    "        cbar = ax.collections[0].colorbar\n",
    "        \n",
    "        #Set font size of colorbar\n",
    "        #cbar.ax.tick_params(labelsize=18)\n",
    "\n",
    "        #Set ticks of colorbar (Texts)\n",
    "        #for index, item in enumerate(colorbarTextPoints):\n",
    "        #    colorbarTextPoints[index] = 2 * colorbarTextPoints[index] - 1\n",
    "        cbar.set_ticks(colorbarTextPoints)\n",
    "        cbar.set_ticklabels(colorbarTexts)\n",
    "        #cbar.ax.set_yticklabels(colorbarTexts, rotation = colorbarTextRotation)  #if 'orientation': 'vertical',\n",
    "        cbar.ax.set_yticklabels(colorbarTexts, rotation = colorbarTextRotation, fontsize=15)  #if 'orientation': 'horizontal',\n",
    "        #cbar.set(size=\"5%\", pad=0.05)\n",
    "        cmap = cbar.cmap\n",
    "        \n",
    "        cmap.set_under('white')#Colour values less than vmin in white\n",
    "        cmap.set_over('yellow')# colour valued larger than vmax in red\n",
    "    \n",
    "    #plt.setp(ax.get_xticklabels(), horizontalalignment='right', fontsize='small')\n",
    "    \n",
    "    plt.show(block=False)\n",
    "    plt.pause(1)\n",
    "    plt.close()\n",
    "\n",
    "    return figure\n",
    "\n",
    "#pipelineExecutionFigure5_v4()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "eastern-howard",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nsuccessPercentageFigure_v4()\\n\\nimport matplotlib.pyplot as plt\\nimport matplotlib as mpl\\n\\n#fig, ax = plt.subplots(figsize=(6, 1))\\n\\nfig = sampleFigure.add_subplot(1, 2, 2)\\n\\nsampleFigure.subplots_adjust(right=0.5)\\n\\ncmap = ax.collections[0].cmap\\nprint(cmap)   \\n\\n\\ncmap = mpl.colors.ListedColormap(['red', 'green', 'blue', 'cyan'])\\n#cmap.set_over('0.25')\\n#cmap.set_under('0.75')\\n\\nbounds = [0.5, 1]\\n#norm = mpl.colors.BoundaryNorm(bounds, cmap.N)\\ncb2 = mpl.colorbar.ColorbarBase(ax, cmap=cmap,\\n                                #norm=norm,\\n                                boundaries=bounds,\\n                                #extend='both',\\n                                ticks=bounds,\\n                                spacing='proportional',\\n                                orientation='horizontal')\\ncb2.set_label('Discrete intervals, some other units')\\n#sampleFigure.draw_wrapper()\\n\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "successPercentageFigure_v4()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "#fig, ax = plt.subplots(figsize=(6, 1))\n",
    "\n",
    "fig = sampleFigure.add_subplot(1, 2, 2)\n",
    "\n",
    "sampleFigure.subplots_adjust(right=0.5)\n",
    "\n",
    "cmap = ax.collections[0].cmap\n",
    "print(cmap)   \n",
    "\n",
    "\n",
    "cmap = mpl.colors.ListedColormap(['red', 'green', 'blue', 'cyan'])\n",
    "#cmap.set_over('0.25')\n",
    "#cmap.set_under('0.75')\n",
    "\n",
    "bounds = [0.5, 1]\n",
    "#norm = mpl.colors.BoundaryNorm(bounds, cmap.N)\n",
    "cb2 = mpl.colorbar.ColorbarBase(ax, cmap=cmap,\n",
    "                                #norm=norm,\n",
    "                                boundaries=bounds,\n",
    "                                #extend='both',\n",
    "                                ticks=bounds,\n",
    "                                spacing='proportional',\n",
    "                                orientation='horizontal')\n",
    "cb2.set_label('Discrete intervals, some other units')\n",
    "#sampleFigure.draw_wrapper()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "planned-point",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nimport matplotlib.pyplot as plt\\nimport matplotlib as mpl\\n\\nimg, axe = plt.subplots(figsize=(6, 1))\\ncmap = mpl.colors.ListedColormap(['red', 'green', 'blue', 'cyan'])\\nbounds = [0, 10]\\n#norm = mpl.colors.BoundaryNorm(bounds, cmap.N)\\ncb2 = mpl.colorbar.ColorbarBase(axe, cmap=cmap,\\n                                #norm=norm,\\n                                boundaries=bounds,\\n                                #extend='both',\\n                                ticks=bounds,\\n                                spacing='proportional',\\n                                orientation='horizontal')\\ncb2.set_label('Discrete intervals, some other units')\\n\""
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "img, axe = plt.subplots(figsize=(6, 1))\n",
    "cmap = mpl.colors.ListedColormap(['red', 'green', 'blue', 'cyan'])\n",
    "bounds = [0, 10]\n",
    "#norm = mpl.colors.BoundaryNorm(bounds, cmap.N)\n",
    "cb2 = mpl.colorbar.ColorbarBase(axe, cmap=cmap,\n",
    "                                #norm=norm,\n",
    "                                boundaries=bounds,\n",
    "                                #extend='both',\n",
    "                                ticks=bounds,\n",
    "                                spacing='proportional',\n",
    "                                orientation='horizontal')\n",
    "cb2.set_label('Discrete intervals, some other units')\n",
    "'''\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
